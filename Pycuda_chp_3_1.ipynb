{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cuda_chp_3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPuVJFuvFzfhcPhjsip3FR5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nile649/CUDA_Tutorials/blob/master/cuda_chp_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-C79OgCZneK"
      },
      "source": [
        "#3 \n",
        "## Getting Started with PyCUDA\n",
        "\n",
        "\n",
        "---\n",
        "We will start\n",
        "by learning how to use PyCUDA for some basic and fundamental operations. We will first\n",
        "see how to query our GPU—that is, we will start by writing a small Python program that\n",
        "will tell us what the characteristics of our GPU are, such as the core count, architecture, and\n",
        "memory. We will then spend some time getting acquainted with how to transfer memory\n",
        "between Python and the GPU with PyCUDA's gpuarray class and how to use this class for\n",
        "basic computations. The remainder of this chapter will be spent showing how to write some\n",
        "basic functions (which we will refer to as CUDA Kernels) that we can directly launch onto\n",
        "the GPU.\n",
        "\n",
        "The learning outcomes for this chapter are as follows:\n",
        "1. Determining GPU characteristics, such as memory capacity or core count, using\n",
        "PyCUDA\n",
        "2. Understanding the difference between host (CPU) and device (GPU) memory\n",
        "and how to use PyCUDA's gpuarray class to transfer data between the host and\n",
        "device\n",
        "3. How to do basic calculations using only gpuarray objects\n",
        "4. How to perform basic element-wise operations on the GPU with the\n",
        "PyCUDA ElementwiseKernel function\n",
        "5. Understanding the functional programming concept of reduce/scan operations\n",
        "and how to make a basic reduction or scan CUDA kernel\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2cIOUELZXFa",
        "outputId": "9b61af1a-af57-48c7-cdcb-f81847310ec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!lscpu\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               63\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "Stepping:            0\n",
            "CPU MHz:             2300.000\n",
            "BogoMIPS:            4600.00\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            256K\n",
            "L3 cache:            46080K\n",
            "NUMA node0 CPU(s):   0,1\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWariYr2ZuCd"
      },
      "source": [
        "Check free memory : !free -g"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_evcjAoZZnG",
        "outputId": "618162e2-a970-47fe-9044-559ee0d17c04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!free -g"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:             12           0          10           0           1          11\n",
            "Swap:             0           0           0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX_DxcmtaHO-"
      },
      "source": [
        "Check GPu card"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ68rbTHZtGR",
        "outputId": "d318b9e4-7f57-40ab-884f-43dff0b40ed3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Oct 12 21:06:35 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcH6GFV5aHX0"
      },
      "source": [
        "# Querying your GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BodN6v2HaHbF"
      },
      "source": [
        "# Installing PyCUDA (Linux)\n",
        "\n",
        "\n",
        "---\n",
        "!pip install PyCUDA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO31A76EcfU1",
        "outputId": "4ba946b6-206c-4b03-d3ad-a4838e8001ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "!pip install PyCUDA"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyCUDA\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/61/47d3235a4c13eec5a5f03594ddb268f4858734e02980afbcd806e6242fa5/pycuda-2020.1.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 8.8MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/d5/989a1d2bba90f5c085e4929a4b703bbd8cc6b4a4218f1671fadab2abe966/pytools-2020.4.tar.gz (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from PyCUDA) (4.4.2)\n",
            "Collecting appdirs>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->PyCUDA) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->PyCUDA) (1.18.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->PyCUDA) (1.1.1)\n",
            "Building wheels for collected packages: PyCUDA, pytools\n",
            "  Building wheel for PyCUDA (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyCUDA: filename=pycuda-2020.1-cp36-cp36m-linux_x86_64.whl size=620945 sha256=8295be418093521305c8a45633909984363011fb8ed6ef140e7b02d5df5c5a41\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/78/d1/5bb826f81d9d490297a348d818ff3ee6dd6f2075b06dde6ea0\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2020.4-py2.py3-none-any.whl size=67175 sha256=ea598d832352509d369a2eaad006b5b698f80781ed1834f50b40197028b60f6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/20/0b/fac51840734b2587ecc239a62522b164c374e929e2c9be66c5\n",
            "Successfully built PyCUDA pytools\n",
            "Installing collected packages: appdirs, pytools, mako, PyCUDA\n",
            "Successfully installed PyCUDA-2020.1 appdirs-1.4.4 mako-1.1.3 pytools-2020.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_cp4-bakPGM",
        "outputId": "41bf5718-2727-4280-f467-4bdbf95b4519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!sudo apt update\n",
        "!sudo add-apt-repository ppa:graphics-drivers\n",
        "!sudo apt-key adv --fetch-keys  http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
        "!sudo bash -c 'echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/cuda.list'\n",
        "!sudo bash -c 'echo \"deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/cuda_learn.list'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\u001b[0m\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.38)] [Co\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.38)] [Co\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [Connecting to security.ubu\u001b[0m\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [3 InRelease 15.6 kB/88.7 kB 18%] [Connecting to \u001b[0m\r                                                                               \rHit:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [3 InRelease 15.6 kB/88.7 kB 18%] [Connecting to \u001b[0m\r                                                                               \rGet:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,681 kB]\n",
            "Ign:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [860 kB]\n",
            "Get:15 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [39.1 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [220 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,110 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,104 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [45.2 kB]\n",
            "Ign:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [334 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,693 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,341 kB]\n",
            "Fetched 10.7 MB in 2s (5,328 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "17 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            " Fresh drivers from upstream, currently shipping Nvidia.\n",
            "\n",
            "## Current Status\n",
            "\n",
            "Current long-lived branch release: `nvidia-430` (430.40)\n",
            "Dropped support for Fermi series (https://nvidia.custhelp.com/app/answers/detail/a_id/4656)\n",
            "\n",
            "Old long-lived branch release: `nvidia-390` (390.129)\n",
            "\n",
            "For GF1xx GPUs use `nvidia-390` (390.129)\n",
            "For G8x, G9x and GT2xx GPUs use `nvidia-340` (340.107)\n",
            "For NV4x and G7x GPUs use `nvidia-304` (304.137) End-Of-Life!\n",
            "\n",
            "Support timeframes for Unix legacy GPU releases:\n",
            "https://nvidia.custhelp.com/app/answers/detail/a_id/3142\n",
            "\n",
            "## What we're working on right now:\n",
            "\n",
            "- Normal driver updates\n",
            "- Help Wanted: Mesa Updates for Intel/AMD users, ping us if you want to help do this work, we're shorthanded.\n",
            "\n",
            "## WARNINGS:\n",
            "\n",
            "This PPA is currently in testing, you should be experienced with packaging before you dive in here:\n",
            "\n",
            "Volunteers welcome!\n",
            "\n",
            "### How you can help:\n",
            "\n",
            "## Install PTS and benchmark your gear:\n",
            "\n",
            "    sudo apt-get install phoronix-test-suite\n",
            "\n",
            "Run the benchmark:\n",
            "\n",
            "    phoronix-test-suite default-benchmark openarena xonotic tesseract gputest unigine-valley\n",
            "\n",
            "and then say yes when it asks you to submit your results to openbechmarking.org. Then grab a cup of coffee, it takes a bit for the benchmarks to run. Depending on the version of Ubuntu you're using it might preferable for you to grabs PTS from upstream directly: http://www.phoronix-test-suite.com/?k=downloads\n",
            "\n",
            "## Share your results with the community:\n",
            "\n",
            "Post a link to your results (or any other feedback to): https://launchpad.net/~graphics-drivers-testers\n",
            "\n",
            "Remember to rerun and resubmit the benchmarks after driver upgrades, this will allow us to gather a bunch of data on performance that we can share with everybody.\n",
            "\n",
            "If you run into old documentation referring to other PPAs, you can help us by consolidating references to this PPA.\n",
            "\n",
            "If someone wants to go ahead and start prototyping on `software-properties-gtk` on what the GUI should look like, please start hacking!\n",
            "\n",
            "## Help us Help You!\n",
            "\n",
            "We use the donation funds to get the developers hardware to test and upload these drivers, please consider donating to the \"community\" slider on the donation page if you're loving this PPA:\n",
            "\n",
            "http://www.ubuntu.com/download/desktop/contribute\n",
            " More info: https://launchpad.net/~graphics-drivers/+archive/ubuntu/ppa\n",
            "Press [ENTER] to continue or Ctrl-c to cancel adding it.\n",
            "\n",
            "Hit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Reading package lists... Done\n",
            "Executing: /tmp/apt-key-gpghome.DTUU4vi1mr/gpg.1.sh --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
            "gpg: requesting key from 'http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub'\n",
            "gpg: key F60F4B3D7FA2AF80: \"cudatools <cudatools@nvidia.com>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhHtl8u1Mil9",
        "outputId": "88cb3234-f8ff-4585-a7ca-68ad934251bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!sudo apt install cuda-10-1\n",
        "!sudo apt install libcudnn7\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cuda-10-1 is already the newest version (10.1.243-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/cuda_learn.list:1 and /etc/apt/sources.list.d/nvidia-ml.list:1\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcudnn7-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn7\n",
            "The following packages will be upgraded:\n",
            "  libcudnn7 libcudnn7-dev\n",
            "2 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/cuda_learn.list:1 and /etc/apt/sources.list.d/nvidia-ml.list:1\n",
            "E: Held packages were changed and -y was used without --allow-change-held-packages.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jduX33bXe99u"
      },
      "source": [
        "code to check GPU specification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4ssBcxlchRY",
        "outputId": "445ba4fd-8f74-4055-c9c4-0a2cfbc6a120",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "import pycuda\n",
        "import pycuda.driver as drv\n",
        "drv.init()\n",
        "\n",
        "print('CUDA device query (PyCUDA version) \\n')\n",
        "\n",
        "print('Detected {} CUDA Capable device(s) \\n'.format(drv.Device.count()))\n",
        "\n",
        "for i in range(drv.Device.count()):\n",
        "    \n",
        "    gpu_device = drv.Device(i)\n",
        "    print('Device {}: {}'.format( i, gpu_device.name() ))\n",
        "    compute_capability = float( '%d.%d' % gpu_device.compute_capability() )\n",
        "    print('\\t Compute Capability: {}'.format(compute_capability))\n",
        "    print('\\t Total Memory: {} megabytes'.format(gpu_device.total_memory()//(1024**2)))\n",
        "    \n",
        "    # The following will give us all remaining device attributes as seen \n",
        "    # in the original deviceQuery.\n",
        "    # We set up a dictionary as such so that we can easily index\n",
        "    # the values using a string descriptor.\n",
        "    \n",
        "    device_attributes_tuples = gpu_device.get_attributes().items() \n",
        "    device_attributes = {}\n",
        "    \n",
        "    for k, v in device_attributes_tuples:\n",
        "        device_attributes[str(k)] = v\n",
        "    \n",
        "    num_mp = device_attributes['MULTIPROCESSOR_COUNT']\n",
        "    \n",
        "    # Cores per multiprocessor is not reported by the GPU!  \n",
        "    # We must use a lookup table based on compute capability.\n",
        "    # See the following:\n",
        "    # http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities\n",
        "    \n",
        "    cuda_cores_per_mp = { 5.0 : 128, 5.1 : 128, 5.2 : 128, 6.0 : 64, 6.1 : 128, 6.2 : 128}[compute_capability]\n",
        "    \n",
        "    print('\\t ({}) Multiprocessors, ({}) CUDA Cores / Multiprocessor: {} CUDA Cores'.format(num_mp, cuda_cores_per_mp, num_mp*cuda_cores_per_mp))\n",
        "    \n",
        "    device_attributes.pop('MULTIPROCESSOR_COUNT')\n",
        "    \n",
        "    for k in device_attributes.keys():\n",
        "        print('\\t {}: {}'.format(k, device_attributes[k]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA device query (PyCUDA version) \n",
            "\n",
            "Detected 1 CUDA Capable device(s) \n",
            "\n",
            "Device 0: Tesla T4\n",
            "\t Compute Capability: 7.5\n",
            "\t Total Memory: 15079 megabytes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-bdaeaea59c4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mcuda_cores_per_mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;36m5.0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.1\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.2\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.1\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.2\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_capability\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t ({}) Multiprocessors, ({}) CUDA Cores / Multiprocessor: {} CUDA Cores'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_cores_per_mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcuda_cores_per_mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 7.5"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6JgEyZDeuTO",
        "outputId": "645ca28d-6937-4807-9355-1c50c0bf9006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Gives the number of GPU which supports CUDA\n",
        "drv.Device.count()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5p_017_f3hX"
      },
      "source": [
        "# compute Capability\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The Compute Capability describes the features supported by a CUDA hardware. First CUDA capable hardware like the GeForce 8800 GTX have a compute capability (CC) of 1.0 and recent GeForce like the GTX 480 have a CC of 2.0. Knowing the CC can be useful for understanting why a CUDA based demo can’t start on your system.\n",
        "\n",
        "CUDA SDK 10.0 – 10.2 support for compute capability 3.0 – 7.5 (Kepler, Maxwell, Pascal, Volta, Turing). Last version with support for compute capability 3.x (Kepler). 10.2 \n",
        "\n",
        "Source : wiki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc0vxMC_fLiH",
        "outputId": "1ef1ea54-28f9-4eec-b3b9-ef1073f2ce00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Compute Capability:\n",
        "i=0\n",
        "gpu_device = drv.Device(i)\n",
        "print('Device {}: {}'.format( i, gpu_device.name() ))\n",
        "compute_capability = float( '%d.%d' % gpu_device.compute_capability() )\n",
        "print('\\t Compute Capability: {}'.format(compute_capability))\n",
        "print('\\t Total Memory: {} megabytes'.format(gpu_device.total_memory()//(1024**2)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device 0: Tesla T4\n",
            "\t Compute Capability: 7.5\n",
            "\t Total Memory: 15079 megabytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAWEm0ncfxKZ",
        "outputId": "0ba4d875-3492-49bc-9918-9945fa306ac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = gpu_device.total_memory()//1024 # The memory size is generally in bytes -> KiloBytes\n",
        "x = x/1024 # MB\n",
        "x/1024 # GB"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.726318359375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjGd1gUWki4s"
      },
      "source": [
        "Each Multi-process has Number of CUDA cores.\n",
        "\n",
        "Stream Multiprocess is 54:\n",
        "\n",
        "Each SM has 64 cores:\n",
        "\n",
        "WHich gives 56*64 : 3584 cores\n",
        "\n",
        "High cores don't indeicate better performance across different architecture.\n",
        "\n",
        "Please refer to following links\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "https://www.extremetech.com/extreme/213519-asynchronous-shading-amd-nvidia-and-dx12-what-we-know-so-far\n",
        "\n",
        "https://www.youtube.com/watch?v=JFhG9UntZs4&ab_channel=GregSalazar\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkL2M4rBhWFd",
        "outputId": "2681c73b-e2a7-4672-fcfc-ebb61e0b6bcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "56*64"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3584"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZaUNJIHS9IR"
      },
      "source": [
        "A GPU divides its individual cores up into larger units known as\n",
        "Streaming Multiprocessors (SMs);\n",
        "\n",
        "a GPU device will have several SMs, which will each\n",
        "individually have a particular number of CUDA cores, depending on the compute\n",
        "capability of the device.\n",
        "\n",
        "*** To be clear: the number of cores per multiprocessor is not indicated\n",
        "directly by the GPU—this is given to us implicitly by the compute capability. ***\n",
        "\n",
        "cuda cores != cores\n",
        "since cuda cores is depended on cc.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w35kp_0_U-Md"
      },
      "source": [
        "# Using PyCUDA's gpuarray class\n",
        "\n",
        "Much like how NumPy's array class is the cornerstone of numerical programming within\n",
        "the NumPy environment, PyCUDA's gpuarray class plays an analogously prominent role\n",
        "within GPU programming in Python. This has all of the features you know and love from\n",
        "NumPy—multidimensional vector/matrix/tensor shape structuring, array-slicing, array\n",
        "unraveling, and overloaded operators for point-wise computations (for example, +, -, *, /,\n",
        "and **).\n",
        "Getting Started with PyCUDA Chapter 3\n",
        "[ 45 ]\n",
        "gpuarray is really an indispensable tool for any budding GPU programmer. We will spend\n",
        "this section going over this particular data structure and gaining a strong grasp of it before\n",
        "we move on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtIGicUvVOvQ"
      },
      "source": [
        "## Transferring data to and from the GPU with gpuarray\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "GPU memory is called Global device memory, whereas CPU is device memory. GPU array is essentially Numpy data structure for CUDA.\n",
        "\n",
        "For the most part, we treat (global) device memory on the GPU as we do\n",
        "dynamically allocated heap memory in C (with the malloc and free functions) or C++ (as\n",
        "with the new and delete operators); in CUDA C, this is complicated further with the\n",
        "additional task of transferring data back and forth between the CPU to the GPU (with\n",
        "commands such as cudaMemcpyHostToDevice and cudaMemcpyDeviceToHost), all while\n",
        "keeping track of multiple pointers in both the CPU and GPU space and performing proper\n",
        "memory allocations (cudaMalloc) and deallocations (cudaFree).\n",
        "\n",
        "\n",
        "---\n",
        "Fortunately, PyCUDA covers all of the overhead of memory allocation, deallocation, and\n",
        "data transfers with the gpuarray class. As stated, this class acts similarly to NumPy arrays,\n",
        "using vector/ matrix/tensor shape structure information for the data. gpuarray objects\n",
        "even perform automatic cleanup based on the lifetime, so we do not have to worry about\n",
        "freeing any GPU memory stored in a gpuarray object when we are done with it\n",
        "\n",
        "---\n",
        "How exactly do we use this to transfer data from the host to the GPU? First, we must\n",
        "contain our host data in some form of NumPy array (let's call it host_data), and then use\n",
        "the ** *gpuarray.to_gpu(host_data)* ** command to transfer this over to the GPU and create\n",
        "a new GPU array.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ome1Z5aMSjp7",
        "outputId": "9b179464-8453-4be8-a308-fa06283c616c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pycuda.autoinit\n",
        "from pycuda import gpuarray\n",
        "\n",
        "host_data = np.array([1,2,3,4,5],dtype=np.float32)\n",
        "device_data = gpuarray.to_gpu(host_data)\n",
        "device_data_ = 19274*device_data\n",
        "device_data_ = device_data_.get()\n",
        "print(device_data_)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19274. 38548. 57822. 77096. 96370.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQdJHA2As5kt"
      },
      "source": [
        "One thing to note is that we specifically denoted that the array on the host had its type\n",
        "specifically set to a NumPy float32 type with the dtype option when we set up our\n",
        "NumPy array; \n",
        "\n",
        "this corresponds directly with the float type in C/C++. Generally speaking,\n",
        "it's a good idea to specifically set data types with NumPy when we are sending data to the\n",
        "GPU. \n",
        "\n",
        "The reason for this is twofold: \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "1. first, since we are using a GPU for increasing the\n",
        "performance of our application, we don't want any unnecessary overhead of using an\n",
        "unnecessary type that will possibly take up more computational time or memory, and\n",
        "\n",
        "2. second, since we will soon be writing portions of code in inline CUDA C, we will have to be\n",
        "very specific with types or our code won't work correctly, keeping in mind that C is a\n",
        "statically-typed language.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Remember to specifically set data types for NumPy arrays that will be\n",
        "transferred to the GPU. This can be done with the dtype option in the\n",
        "constructor of the numpy.array class.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQEereR0uATa"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "#Basic pointwise arithmetic operations with gpuarray\n",
        "\n",
        "---\n",
        "note that a pointwise operation is intrinsically parallelizable, and so when we use this\n",
        "operation on a gpuarray object PyCUDA is able to offload each multiplication operation\n",
        "onto a single thread, rather than computing each multiplication in serial, one after the other\n",
        "(in fairness, some versions of NumPy can use the advanced SSE instructions found in\n",
        "modern x86 chips for these computations, so in some cases the performance will be\n",
        "comparable to a GPU). To be clear: these pointwise operations performed on the GPU are in\n",
        "parallel since the computation of one element is not dependent on the computation of any\n",
        "other element.\n",
        "\n",
        "\n",
        "---\n",
        "# Speed Test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IidT2SBWrmqo",
        "outputId": "7c2cc5de-c59e-470d-e0c2-cb5d5deacd25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pycuda.autoinit\n",
        "from pycuda import gpuarray\n",
        "from time import time\n",
        "host_data = np.float32( np.random.random(50000000) )\n",
        "\n",
        "t1 = time()\n",
        "host_data_2x = host_data * np.float32(2)\n",
        "t2 = time()\n",
        "print('total time to compute on CPU: %f' % (t2 - t1))\n",
        "device_data = gpuarray.to_gpu(host_data)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total time to compute on CPU: 0.035629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viF8dfg7yszM",
        "outputId": "dc651f59-d38d-4f3b-f391-d7d8e0dd32c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "t1 = time()\n",
        "device_data_2x = device_data * np.float32( 2 )\n",
        "t2 = time()\n",
        "from_device = device_data_2x.get()\n",
        "print('total time to compute on GPU: %f' % (t2 - t1))\n",
        "print('Is the host computation the same as the GPU computation? :\\\n",
        "{}'.format(np.allclose(from_device, host_data_2x) ))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total time to compute on GPU: 0.001246\n",
            "Is the host computation the same as the GPU computation? :True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6Sw9vTgzRBT"
      },
      "source": [
        "def func():\n",
        "  host_data = np.float32( np.random.random(50000000) )\n",
        "\n",
        "  t1 = time()\n",
        "  host_data_2x = host_data * np.float32(2)\n",
        "  t2 = time()\n",
        "  print('total time to compute on CPU: %f' % (t2 - t1))\n",
        "  device_data = gpuarray.to_gpu(host_data)\n",
        "  t1 = time()\n",
        "  device_data_2x = device_data * np.float32( 2 )\n",
        "  t2 = time()\n",
        "  from_device = device_data_2x.get()\n",
        "  print('total time to compute on GPU: %f' % (t2 - t1))\n",
        "  print('Is the host computation the same as the GPU computation? :\\\n",
        "  {}'.format(np.allclose(from_device, host_data_2x) ))  "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_tE-8Ot0Lo-"
      },
      "source": [
        "%load_ext line_profiler\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1m2k0Gt0rRE",
        "outputId": "4d493c81-42e0-439b-c47a-cdba7e6d46c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%lprun -f func func()\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total time to compute on CPU: 0.037419\n",
            "total time to compute on GPU: 0.000987\n",
            "Is the host computation the same as the GPU computation? :  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gemmVYdN1fVN"
      },
      "source": [
        "**In PyCUDA, GPU code is often compiled at runtime with the\n",
        "NVIDIA nvcc compiler and then subsequently called from PyCUDA. This\n",
        "can lead to an unexpected slowdown, usually the first time a program or\n",
        "GPU operation is run in a given Python session.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih48_HeK1nwk"
      },
      "source": [
        "# Using PyCUDA's ElementWiseKernel for performing pointwise computations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB7eKUhQ0u05"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
