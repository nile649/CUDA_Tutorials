{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cuda chp 2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOXGb2fdpk/0/mWJVXHIAno",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nile649/CUDA_Tutorials/blob/master/cuda_chp_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-C79OgCZneK"
      },
      "source": [
        "Check CPU feature : !lscpu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2cIOUELZXFa",
        "outputId": "2db8d7d9-0250-45f0-bb03-ffeec78df0bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!lscpu\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               85\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "Stepping:            3\n",
            "CPU MHz:             2000.172\n",
            "BogoMIPS:            4000.34\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            1024K\n",
            "L3 cache:            39424K\n",
            "NUMA node0 CPU(s):   0,1\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWariYr2ZuCd"
      },
      "source": [
        "Check free memory : !free -g"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_evcjAoZZnG",
        "outputId": "6f2263fe-44da-409d-9fde-70c2f5cb2c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!free -g"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:             12           0          10           0           1          11\n",
            "Swap:             0           0           0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX_DxcmtaHO-"
      },
      "source": [
        "Check GPu card"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ68rbTHZtGR",
        "outputId": "53e14e30-4bb1-4fc8-a5ff-2d40f73d9ba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Oct 11 20:45:21 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcH6GFV5aHX0"
      },
      "source": [
        "# Setting up a C++ programming environment\n",
        "In the case of Ubuntu Linux users, the\n",
        "standard repository compilers and IDEs generally work and integrate perfectly with the\n",
        "CUDA Toolkit, while Windows users might have to exercise a little more caution.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Setting up GCC, Eclipse IDE, and graphical\n",
        "dependencies (Linux)\n",
        "Open up a Terminal from the Ubuntu desktop (Ctrl + Alt + T). We first update the\n",
        "apt repository as follows:\n",
        "sudo apt-get update\n",
        "Now we can install everything we need for CUDA with one additional line:\n",
        "sudo apt-get install build-essential binutils gdb eclipse-cdt\n",
        "Here, build-essential is the package with the gcc and g++ compilers, and other utilities\n",
        "such as make; binutils has some generally useful utilities, such as the LD linker, gdb is\n",
        "the debugger, and Eclipse is the IDE that we will be using.\n",
        "Let's also install a few additional dependencies that will allow us to run some of the\n",
        "graphical (OpenGL) demos included with the CUDA Toolkit with this line:\n",
        "sudo apt-get install freeglut3 freeglut3-dev libxi-dev libxmu-dev\n",
        "Now you should be good to go to install the CUDA Toolkit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BodN6v2HaHbF"
      },
      "source": [
        "# Installing PyCUDA (Linux)\n",
        "\n",
        "\n",
        "---\n",
        "!pip install PyCUDA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO31A76EcfU1",
        "outputId": "0bb5bbed-f097-4667-90df-8b1b4b45bed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "!pip install PyCUDA"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyCUDA\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/61/47d3235a4c13eec5a5f03594ddb268f4858734e02980afbcd806e6242fa5/pycuda-2020.1.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 7.3MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/d5/989a1d2bba90f5c085e4929a4b703bbd8cc6b4a4218f1671fadab2abe966/pytools-2020.4.tar.gz (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from PyCUDA) (4.4.2)\n",
            "Collecting appdirs>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->PyCUDA) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->PyCUDA) (1.18.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->PyCUDA) (1.1.1)\n",
            "Building wheels for collected packages: PyCUDA, pytools\n",
            "  Building wheel for PyCUDA (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyCUDA: filename=pycuda-2020.1-cp36-cp36m-linux_x86_64.whl size=620940 sha256=0105a3d88d24e8217a1c75f6a7dc6e14025f8c60f556b88e29b950638135f89e\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/78/d1/5bb826f81d9d490297a348d818ff3ee6dd6f2075b06dde6ea0\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2020.4-py2.py3-none-any.whl size=67175 sha256=b59f7c7e549184d62690f02d9a162f9ecdb7329c0c99fdf43c7e2f8d781cd25a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/20/0b/fac51840734b2587ecc239a62522b164c374e929e2c9be66c5\n",
            "Successfully built PyCUDA pytools\n",
            "Installing collected packages: appdirs, pytools, mako, PyCUDA\n",
            "Successfully installed PyCUDA-2020.1 appdirs-1.4.4 mako-1.1.3 pytools-2020.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jduX33bXe99u"
      },
      "source": [
        "code to check GPU specification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4ssBcxlchRY",
        "outputId": "95e8c85d-cd7f-4317-a735-ed1adb8eab16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pycuda\n",
        "import pycuda.driver as drv\n",
        "drv.init()\n",
        "\n",
        "print('CUDA device query (PyCUDA version) \\n')\n",
        "\n",
        "print('Detected {} CUDA Capable device(s) \\n'.format(drv.Device.count()))\n",
        "\n",
        "for i in range(drv.Device.count()):\n",
        "    \n",
        "    gpu_device = drv.Device(i)\n",
        "    print('Device {}: {}'.format( i, gpu_device.name() ))\n",
        "    compute_capability = float( '%d.%d' % gpu_device.compute_capability() )\n",
        "    print('\\t Compute Capability: {}'.format(compute_capability))\n",
        "    print('\\t Total Memory: {} megabytes'.format(gpu_device.total_memory()//(1024**2)))\n",
        "    \n",
        "    # The following will give us all remaining device attributes as seen \n",
        "    # in the original deviceQuery.\n",
        "    # We set up a dictionary as such so that we can easily index\n",
        "    # the values using a string descriptor.\n",
        "    \n",
        "    device_attributes_tuples = gpu_device.get_attributes().items() \n",
        "    device_attributes = {}\n",
        "    \n",
        "    for k, v in device_attributes_tuples:\n",
        "        device_attributes[str(k)] = v\n",
        "    \n",
        "    num_mp = device_attributes['MULTIPROCESSOR_COUNT']\n",
        "    \n",
        "    # Cores per multiprocessor is not reported by the GPU!  \n",
        "    # We must use a lookup table based on compute capability.\n",
        "    # See the following:\n",
        "    # http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities\n",
        "    \n",
        "    cuda_cores_per_mp = { 5.0 : 128, 5.1 : 128, 5.2 : 128, 6.0 : 64, 6.1 : 128, 6.2 : 128}[compute_capability]\n",
        "    \n",
        "    print('\\t ({}) Multiprocessors, ({}) CUDA Cores / Multiprocessor: {} CUDA Cores'.format(num_mp, cuda_cores_per_mp, num_mp*cuda_cores_per_mp))\n",
        "    \n",
        "    device_attributes.pop('MULTIPROCESSOR_COUNT')\n",
        "    \n",
        "    for k in device_attributes.keys():\n",
        "        print('\\t {}: {}'.format(k, device_attributes[k]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA device query (PyCUDA version) \n",
            "\n",
            "Detected 1 CUDA Capable device(s) \n",
            "\n",
            "Device 0: Tesla P100-PCIE-16GB\n",
            "\t Compute Capability: 6.0\n",
            "\t Total Memory: 16280 megabytes\n",
            "\t (56) Multiprocessors, (64) CUDA Cores / Multiprocessor: 3584 CUDA Cores\n",
            "\t ASYNC_ENGINE_COUNT: 2\n",
            "\t CAN_MAP_HOST_MEMORY: 1\n",
            "\t CLOCK_RATE: 1328500\n",
            "\t COMPUTE_CAPABILITY_MAJOR: 6\n",
            "\t COMPUTE_CAPABILITY_MINOR: 0\n",
            "\t COMPUTE_MODE: DEFAULT\n",
            "\t CONCURRENT_KERNELS: 1\n",
            "\t ECC_ENABLED: 1\n",
            "\t GLOBAL_L1_CACHE_SUPPORTED: 1\n",
            "\t GLOBAL_MEMORY_BUS_WIDTH: 4096\n",
            "\t GPU_OVERLAP: 1\n",
            "\t INTEGRATED: 0\n",
            "\t KERNEL_EXEC_TIMEOUT: 0\n",
            "\t L2_CACHE_SIZE: 4194304\n",
            "\t LOCAL_L1_CACHE_SUPPORTED: 1\n",
            "\t MANAGED_MEMORY: 1\n",
            "\t MAXIMUM_SURFACE1D_LAYERED_LAYERS: 2048\n",
            "\t MAXIMUM_SURFACE1D_LAYERED_WIDTH: 32768\n",
            "\t MAXIMUM_SURFACE1D_WIDTH: 32768\n",
            "\t MAXIMUM_SURFACE2D_HEIGHT: 65536\n",
            "\t MAXIMUM_SURFACE2D_LAYERED_HEIGHT: 32768\n",
            "\t MAXIMUM_SURFACE2D_LAYERED_LAYERS: 2048\n",
            "\t MAXIMUM_SURFACE2D_LAYERED_WIDTH: 32768\n",
            "\t MAXIMUM_SURFACE2D_WIDTH: 131072\n",
            "\t MAXIMUM_SURFACE3D_DEPTH: 16384\n",
            "\t MAXIMUM_SURFACE3D_HEIGHT: 16384\n",
            "\t MAXIMUM_SURFACE3D_WIDTH: 16384\n",
            "\t MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS: 2046\n",
            "\t MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH: 32768\n",
            "\t MAXIMUM_SURFACECUBEMAP_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE1D_LAYERED_LAYERS: 2048\n",
            "\t MAXIMUM_TEXTURE1D_LAYERED_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE1D_LINEAR_WIDTH: 134217728\n",
            "\t MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH: 16384\n",
            "\t MAXIMUM_TEXTURE1D_WIDTH: 131072\n",
            "\t MAXIMUM_TEXTURE2D_ARRAY_HEIGHT: 32768\n",
            "\t MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES: 2048\n",
            "\t MAXIMUM_TEXTURE2D_ARRAY_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE2D_GATHER_HEIGHT: 32768\n",
            "\t MAXIMUM_TEXTURE2D_GATHER_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE2D_HEIGHT: 65536\n",
            "\t MAXIMUM_TEXTURE2D_LINEAR_HEIGHT: 65000\n",
            "\t MAXIMUM_TEXTURE2D_LINEAR_PITCH: 2097120\n",
            "\t MAXIMUM_TEXTURE2D_LINEAR_WIDTH: 131072\n",
            "\t MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT: 32768\n",
            "\t MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE2D_WIDTH: 131072\n",
            "\t MAXIMUM_TEXTURE3D_DEPTH: 16384\n",
            "\t MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE: 32768\n",
            "\t MAXIMUM_TEXTURE3D_HEIGHT: 16384\n",
            "\t MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE: 8192\n",
            "\t MAXIMUM_TEXTURE3D_WIDTH: 16384\n",
            "\t MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE: 8192\n",
            "\t MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS: 2046\n",
            "\t MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURECUBEMAP_WIDTH: 32768\n",
            "\t MAX_BLOCK_DIM_X: 1024\n",
            "\t MAX_BLOCK_DIM_Y: 1024\n",
            "\t MAX_BLOCK_DIM_Z: 64\n",
            "\t MAX_GRID_DIM_X: 2147483647\n",
            "\t MAX_GRID_DIM_Y: 65535\n",
            "\t MAX_GRID_DIM_Z: 65535\n",
            "\t MAX_PITCH: 2147483647\n",
            "\t MAX_REGISTERS_PER_BLOCK: 65536\n",
            "\t MAX_REGISTERS_PER_MULTIPROCESSOR: 65536\n",
            "\t MAX_SHARED_MEMORY_PER_BLOCK: 49152\n",
            "\t MAX_SHARED_MEMORY_PER_MULTIPROCESSOR: 65536\n",
            "\t MAX_THREADS_PER_BLOCK: 1024\n",
            "\t MAX_THREADS_PER_MULTIPROCESSOR: 2048\n",
            "\t MEMORY_CLOCK_RATE: 715000\n",
            "\t MULTI_GPU_BOARD: 0\n",
            "\t MULTI_GPU_BOARD_GROUP_ID: 0\n",
            "\t PCI_BUS_ID: 0\n",
            "\t PCI_DEVICE_ID: 4\n",
            "\t PCI_DOMAIN_ID: 0\n",
            "\t STREAM_PRIORITIES_SUPPORTED: 1\n",
            "\t SURFACE_ALIGNMENT: 512\n",
            "\t TCC_DRIVER: 0\n",
            "\t TEXTURE_ALIGNMENT: 512\n",
            "\t TEXTURE_PITCH_ALIGNMENT: 32\n",
            "\t TOTAL_CONSTANT_MEMORY: 65536\n",
            "\t UNIFIED_ADDRESSING: 1\n",
            "\t WARP_SIZE: 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6JgEyZDeuTO",
        "outputId": "2cf735ad-23a3-4f52-ea3d-0eb56ec7f87e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Gives the number of GPU which supports CUDA\n",
        "drv.Device.count()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5p_017_f3hX"
      },
      "source": [
        "# compute Capability\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The Compute Capability describes the features supported by a CUDA hardware. First CUDA capable hardware like the GeForce 8800 GTX have a compute capability (CC) of 1.0 and recent GeForce like the GTX 480 have a CC of 2.0. Knowing the CC can be useful for understanting why a CUDA based demo can’t start on your system.\n",
        "\n",
        "CUDA SDK 10.0 – 10.2 support for compute capability 3.0 – 7.5 (Kepler, Maxwell, Pascal, Volta, Turing). Last version with support for compute capability 3.x (Kepler). 10.2 \n",
        "\n",
        "Source : wiki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc0vxMC_fLiH",
        "outputId": "b7c05e81-a131-47c0-f450-39f64d39d760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Compute Capability:\n",
        "i=0\n",
        "gpu_device = drv.Device(i)\n",
        "print('Device {}: {}'.format( i, gpu_device.name() ))\n",
        "compute_capability = float( '%d.%d' % gpu_device.compute_capability() )\n",
        "print('\\t Compute Capability: {}'.format(compute_capability))\n",
        "print('\\t Total Memory: {} megabytes'.format(gpu_device.total_memory()//(1024**2)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device 0: Tesla P100-PCIE-16GB\n",
            "\t Compute Capability: 6.0\n",
            "\t Total Memory: 16280 megabytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAWEm0ncfxKZ",
        "outputId": "c5399646-82a2-4396-d57c-0cfcd937a656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = gpu_device.total_memory()//1024 # The memory size is generally in bytes -> KiloBytes\n",
        "x = x/1024 # MB\n",
        "x/1024 # GB"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.8992919921875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjGd1gUWki4s"
      },
      "source": [
        "Each Multi-process has Number of CUDA cores.\n",
        "\n",
        "Stream Multiprocess is 54:\n",
        "\n",
        "Each SM has 64 cores:\n",
        "\n",
        "WHich gives 56*64 : 3584 cores\n",
        "\n",
        "High cores don't indeicate better performance across different architecture.\n",
        "\n",
        "Please refer to following links\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "https://www.extremetech.com/extreme/213519-asynchronous-shading-amd-nvidia-and-dx12-what-we-know-so-far\n",
        "\n",
        "https://www.youtube.com/watch?v=JFhG9UntZs4&ab_channel=GregSalazar\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkL2M4rBhWFd",
        "outputId": "2681c73b-e2a7-4672-fcfc-ebb61e0b6bcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "56*64"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3584"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_cp4-bakPGM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
