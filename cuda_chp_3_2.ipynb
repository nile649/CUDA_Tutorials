{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cuda_chp_3_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLV0DMmXer5ouMclgl4LTG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nile649/CUDA_Tutorials/blob/master/cuda_chp_3_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-C79OgCZneK"
      },
      "source": [
        "#3 | 2\n",
        "## Getting Started with PyCUDA\n",
        "\n",
        "\n",
        "---\n",
        "We will start\n",
        "by learning how to use PyCUDA for some basic and fundamental operations. We will first\n",
        "see how to query our GPU—that is, we will start by writing a small Python program that\n",
        "will tell us what the characteristics of our GPU are, such as the core count, architecture, and\n",
        "memory. We will then spend some time getting acquainted with how to transfer memory\n",
        "between Python and the GPU with PyCUDA's gpuarray class and how to use this class for\n",
        "basic computations. The remainder of this chapter will be spent showing how to write some\n",
        "basic functions (which we will refer to as CUDA Kernels) that we can directly launch onto\n",
        "the GPU.\n",
        "\n",
        "The learning outcomes for this chapter are as follows:\n",
        "1. Determining GPU characteristics, such as memory capacity or core count, using\n",
        "PyCUDA\n",
        "2. Understanding the difference between host (CPU) and device (GPU) memory\n",
        "and how to use PyCUDA's gpuarray class to transfer data between the host and\n",
        "device\n",
        "3. How to do basic calculations using only gpuarray objects\n",
        "4. How to perform basic element-wise operations on the GPU with the\n",
        "PyCUDA ElementwiseKernel function\n",
        "5. Understanding the functional programming concept of reduce/scan operations\n",
        "and how to make a basic reduction or scan CUDA kernel\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2cIOUELZXFa",
        "outputId": "bf9ef9c7-bca4-4d91-d27e-3d1261714bc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!lscpu\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               85\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "Stepping:            3\n",
            "CPU MHz:             2000.162\n",
            "BogoMIPS:            4000.32\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            1024K\n",
            "L3 cache:            39424K\n",
            "NUMA node0 CPU(s):   0,1\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWariYr2ZuCd"
      },
      "source": [
        "Check free memory : !free -g"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_evcjAoZZnG",
        "outputId": "95f1fe7a-f95d-4cad-cde5-ad8e5931c85b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!free -g"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:             12           0          10           0           1          11\n",
            "Swap:             0           0           0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX_DxcmtaHO-"
      },
      "source": [
        "Check GPu card"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ68rbTHZtGR",
        "outputId": "7e84d568-9942-44f9-85e4-5fabaaaa296e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Oct 17 20:34:15 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcH6GFV5aHX0"
      },
      "source": [
        "# Querying your GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BodN6v2HaHbF"
      },
      "source": [
        "# Installing PyCUDA (Linux)\n",
        "\n",
        "\n",
        "---\n",
        "!pip install PyCUDA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO31A76EcfU1",
        "outputId": "94649c4b-fb8e-454f-c6b9-347de051a22a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "!pip install PyCUDA"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyCUDA\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/61/47d3235a4c13eec5a5f03594ddb268f4858734e02980afbcd806e6242fa5/pycuda-2020.1.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 8.4MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/a0/3bb92fa06c46fe9dc55b344e3b43bcee33a2ee106556564ebec9db2a36fd/pytools-2020.4.2.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from PyCUDA) (4.4.2)\n",
            "Collecting appdirs>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->PyCUDA) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->PyCUDA) (1.18.5)\n",
            "Requirement already satisfied: dataclasses>=0.7 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->PyCUDA) (0.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->PyCUDA) (1.1.1)\n",
            "Building wheels for collected packages: PyCUDA, pytools\n",
            "  Building wheel for PyCUDA (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyCUDA: filename=pycuda-2020.1-cp36-cp36m-linux_x86_64.whl size=621391 sha256=9710f974d0c3a8ef8f8a25f2feaadf348f9ecde15b271f043c1ce00361552115\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/78/d1/5bb826f81d9d490297a348d818ff3ee6dd6f2075b06dde6ea0\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2020.4.2-py2.py3-none-any.whl size=61375 sha256=2820558f1aed4d4ee74866ddc9d3b3558f9d46e69051578f5115318c480fe93e\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/d2/2e/fe5677f02563b086529f2422c3fbbf61ee9712249518e9e71e\n",
            "Successfully built PyCUDA pytools\n",
            "Installing collected packages: appdirs, pytools, mako, PyCUDA\n",
            "Successfully installed PyCUDA-2020.1 appdirs-1.4.4 mako-1.1.3 pytools-2020.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_cp4-bakPGM",
        "outputId": "d7de9c81-fe3e-44ac-a56e-5c1fe1ec29c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!sudo apt update\n",
        "!sudo add-apt-repository ppa:graphics-drivers\n",
        "!sudo apt-key adv --fetch-keys  http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
        "!sudo bash -c 'echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/cuda.list'\n",
        "!sudo bash -c 'echo \"deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/cuda_learn.list'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Waiting for headers] [Co\u001b[0m\r                                                                               \rGet:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [2 InRelease 14.2 kB/88.7\u001b[0m\u001b[33m\r0% [Waiting for headers] [2 InRelease 43.1 kB/88.7 kB 49%] [Connected to cloud.\u001b[0m\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [2 InRelease 43.1 kB/88.7 kB 49%] [Connected to cloud.\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [2 InRelease 43.1 kB/88.7 k\u001b[0m\r                                                                               \rGet:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [4 InRelease 14.2 kB/88.7 kB 16%] [2 InRelease 54\u001b[0m\r                                                                               \rHit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [4 InRelease 15.6 kB/88.7 kB 18%] [2 InRelease 75\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [4 InRelease 15.6 kB/88.7 kB 18%] [Waiting for he\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,681 kB]\n",
            "Get:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [860 kB]\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,733 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,348 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [15.0 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,112 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [231 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,150 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [45.5 kB]\n",
            "Ign:23 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:23 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [335 kB]\n",
            "Fetched 10.8 MB in 1s (7,523 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "21 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            " Fresh drivers from upstream, currently shipping Nvidia.\n",
            "\n",
            "## Current Status\n",
            "\n",
            "Current long-lived branch release: `nvidia-430` (430.40)\n",
            "Dropped support for Fermi series (https://nvidia.custhelp.com/app/answers/detail/a_id/4656)\n",
            "\n",
            "Old long-lived branch release: `nvidia-390` (390.129)\n",
            "\n",
            "For GF1xx GPUs use `nvidia-390` (390.129)\n",
            "For G8x, G9x and GT2xx GPUs use `nvidia-340` (340.107)\n",
            "For NV4x and G7x GPUs use `nvidia-304` (304.137) End-Of-Life!\n",
            "\n",
            "Support timeframes for Unix legacy GPU releases:\n",
            "https://nvidia.custhelp.com/app/answers/detail/a_id/3142\n",
            "\n",
            "## What we're working on right now:\n",
            "\n",
            "- Normal driver updates\n",
            "- Help Wanted: Mesa Updates for Intel/AMD users, ping us if you want to help do this work, we're shorthanded.\n",
            "\n",
            "## WARNINGS:\n",
            "\n",
            "This PPA is currently in testing, you should be experienced with packaging before you dive in here:\n",
            "\n",
            "Volunteers welcome!\n",
            "\n",
            "### How you can help:\n",
            "\n",
            "## Install PTS and benchmark your gear:\n",
            "\n",
            "    sudo apt-get install phoronix-test-suite\n",
            "\n",
            "Run the benchmark:\n",
            "\n",
            "    phoronix-test-suite default-benchmark openarena xonotic tesseract gputest unigine-valley\n",
            "\n",
            "and then say yes when it asks you to submit your results to openbechmarking.org. Then grab a cup of coffee, it takes a bit for the benchmarks to run. Depending on the version of Ubuntu you're using it might preferable for you to grabs PTS from upstream directly: http://www.phoronix-test-suite.com/?k=downloads\n",
            "\n",
            "## Share your results with the community:\n",
            "\n",
            "Post a link to your results (or any other feedback to): https://launchpad.net/~graphics-drivers-testers\n",
            "\n",
            "Remember to rerun and resubmit the benchmarks after driver upgrades, this will allow us to gather a bunch of data on performance that we can share with everybody.\n",
            "\n",
            "If you run into old documentation referring to other PPAs, you can help us by consolidating references to this PPA.\n",
            "\n",
            "If someone wants to go ahead and start prototyping on `software-properties-gtk` on what the GUI should look like, please start hacking!\n",
            "\n",
            "## Help us Help You!\n",
            "\n",
            "We use the donation funds to get the developers hardware to test and upload these drivers, please consider donating to the \"community\" slider on the donation page if you're loving this PPA:\n",
            "\n",
            "http://www.ubuntu.com/download/desktop/contribute\n",
            " More info: https://launchpad.net/~graphics-drivers/+archive/ubuntu/ppa\n",
            "Press [ENTER] to continue or Ctrl-c to cancel adding it.\n",
            "\n",
            "Hit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Reading package lists... Done\n",
            "Executing: /tmp/apt-key-gpghome.6d1ZJewWwy/gpg.1.sh --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
            "gpg: requesting key from 'http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub'\n",
            "gpg: key F60F4B3D7FA2AF80: \"cudatools <cudatools@nvidia.com>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhHtl8u1Mil9",
        "outputId": "12b825a5-bc34-4096-87d8-eb50dc132de5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "!sudo apt install cuda-10-1\n",
        "!sudo apt install libcudnn7\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cuda-10-1 is already the newest version (10.1.243-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/cuda_learn.list:1 and /etc/apt/sources.list.d/nvidia-ml.list:1\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcudnn7-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn7\n",
            "The following packages will be upgraded:\n",
            "  libcudnn7 libcudnn7-dev\n",
            "2 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/cuda_learn.list:1 and /etc/apt/sources.list.d/nvidia-ml.list:1\n",
            "E: Held packages were changed and -y was used without --allow-change-held-packages.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jduX33bXe99u"
      },
      "source": [
        "code to check GPU specification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih48_HeK1nwk"
      },
      "source": [
        "# Using PyCUDA's ElementWiseKernel for performing pointwise computations\n",
        "\n",
        "\n",
        "---\n",
        "We use the term kernel quite a bit in this text; by kernel, we always mean a function that is\n",
        "launched directly onto the GPU by CUDA. We will use several functions from PyCUDA\n",
        "that generate templates and design patterns for different types of kernels, easing our\n",
        "transition into GPU programming.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB7eKUhQ0u05",
        "outputId": "63b12cf6-25f9-4220-e8e8-33c5c210477a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "import numpy as np\n",
        "import pycuda.autoinit as ai\n",
        "from pycuda import gpuarray\n",
        "import timeit\n",
        "from pycuda.elementwise import ElementwiseKernel\n",
        "\n",
        "'''\n",
        "ElementwiseKernel::\n",
        "\n",
        "class pycuda.elementwise.ElementwiseKernel(arguments, operation, name='kernel', keep=False, options=[], preamble='')\n",
        "\n",
        "Generate a kernel that takes a number of scalar or vector arguments and performs the scalar operation on each entry of its arguments, if that argument is a vector.\n",
        "\n",
        "arguments is specified as a string formatted as a C argument list. operation is specified as a C assignment statement, without a semicolon. Vectors in operation should be indexed by the variable i.\n",
        "\n",
        "name specifies the name as which the kernel is compiled, keep and options are passed unmodified to pycuda.compiler.SourceModule.\n",
        "\n",
        "preamble specifies some source code that is included before the elementwise kernel specification. You may use this to include other files and/or define functions that are used by operation.\n",
        "\n",
        "__call__(*args, range=None, slice=None)\n",
        "Invoke the generated scalar kernel. The arguments may either be scalars or GPUArray instances.\n",
        "\n",
        "If range is given, it must be a slice object and specifies the range of indices i for which the operation is carried out.\n",
        "\n",
        "If slice is given, it must be a slice object and specifies the range of indices i for which the operation is carried out, truncated to the container. Also, slice may contain negative indices to index relative to the end of the array.\n",
        "\n",
        "If stream is given, it must be a pycuda.driver.Stream object, where the execution will be serialized.\n",
        "\n",
        "'''\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nElementwiseKernel::\\n\\nclass pycuda.elementwise.ElementwiseKernel(arguments, operation, name='kernel', keep=False, options=[], preamble='')\\n\\nGenerate a kernel that takes a number of scalar or vector arguments and performs the scalar operation on each entry of its arguments, if that argument is a vector.\\n\\narguments is specified as a string formatted as a C argument list. operation is specified as a C assignment statement, without a semicolon. Vectors in operation should be indexed by the variable i.\\n\\nname specifies the name as which the kernel is compiled, keep and options are passed unmodified to pycuda.compiler.SourceModule.\\n\\npreamble specifies some source code that is included before the elementwise kernel specification. You may use this to include other files and/or define functions that are used by operation.\\n\\n__call__(*args, range=None, slice=None)\\nInvoke the generated scalar kernel. The arguments may either be scalars or GPUArray instances.\\n\\nIf range is given, it must be a slice object and specifies the range of indices i for which the operation is carried out.\\n\\nIf slice is given, it must be a slice object and specifies the range of indices i for which the operation is carried out, truncated to the container. Also, slice may contain negative indices to index relative to the end of the array.\\n\\nIf stream is given, it must be a pycuda.driver.Stream object, where the execution will be serialized.\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_fQOjxMVoLw",
        "outputId": "73934035-a35b-416b-c4ca-c02fd4e4f298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "host_data = np.float32(np.random.randn(50000000))\n",
        "gpu_2x_eln_ker = ElementwiseKernel(\"float *in, float *out\", \"out[i] = 2*in[i];\",\"gpu_2x_eln_ker\")\n",
        "\n",
        "'''\n",
        "Note that PyCUDA automatically sets up the integer index i for us. When we use i as our\n",
        "index, ElementwiseKernel will automatically parallelize our calculation over i among\n",
        "the many cores in our GPU.\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nNote that PyCUDA automatically sets up the integer index i for us. When we use i as our\\nindex, ElementwiseKernel will automatically parallelize our calculation over i among\\nthe many cores in our GPU.\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjaGT-32V9bz"
      },
      "source": [
        "def speedcomparison():\n",
        "  tic = timeit.default_timer()\n",
        "  host_data_2x = 2*host_data\n",
        "  toc = timeit.default_timer()\n",
        "  print(\"Total time for numpy scalar multiplication {}\".format(toc-tic))\n",
        "  # tic = timeit.default_timer()\n",
        "  device_data_2x = gpuarray.to_gpu(host_data)\n",
        "  device_data_2x_res = gpuarray.empty_like(device_data_2x)\n",
        "  tic = timeit.default_timer()\n",
        "\n",
        "  ###########********************************************************************************\n",
        "  gpu_2x_eln_ker(device_data_2x,device_data_2x_res)\n",
        "  ###########********************************************************************************\n",
        "  toc = timeit.default_timer()\n",
        "  device_data_2x_res_host = device_data_2x_res.get()\n",
        "  # toc = timeit.default_timer()\n",
        "  print(\"Total time for GPU scalar multiplication {}\".format(toc-tic))\n",
        "  print('Is the host computation the same as the GPU computation? :\\\n",
        "  {}'.format(np.allclose(host_data_2x, device_data_2x_res_host) ))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xudVl88GaNrw",
        "outputId": "783395a7-7e03-4266-9c0c-c6562d0b0011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "speedcomparison()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total time for numpy scalar multiplication 0.03906610499961971\n",
            "Total time for GPU scalar multiplication 7.882699992478592e-05\n",
            "Is the host computation the same as the GPU computation? :  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyaCzjYlbaaI"
      },
      "source": [
        "**Now, let's cover something else important before we move on, which is very subtle. The\n",
        "little kernel function we defined operates on C float pointers; this means that we will have\n",
        "to allocate some empty memory on the GPU that is pointed to by the out variable.**\n",
        "\n",
        "---\n",
        "*ElementwiseKernel(\"float *in, float *out\", \"out[i] = 2*in[i];\",\"gpu_2x_eln_ker\")\n",
        "\n",
        "*2nd_arg = gpuarray.empty_like(device_data_2x)\n",
        "gpu_2x_eln_ker(1st_arg,2nd_arg)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwVv-kPcnp5M"
      },
      "source": [
        "# When you allocate memory on the GPU with the PyCUDA functions gpuarray.empty or gpuarray.empty_like, you do not have to deallocate this memory later due to the destructor of the gpuarray object managing all memory clean up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr5IySgraPul",
        "outputId": "fdbe7e68-dbee-481b-c709-76cd41c20bf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "2*2"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO6aN4Gbe_1g"
      },
      "source": [
        "import numpy as np\n",
        "import pycuda.autoinit as ai\n",
        "from pycuda import gpuarray\n",
        "from pycuda.elementwise import ElementwiseKernel\n",
        "import timeit\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mt0q2OumG3O"
      },
      "source": [
        "mandel_ker = ElementwiseKernel(\n",
        "\"pycuda::complex<float> *lattice, float *mandelbrot_graph, int max_iters, float upper_bound\",\n",
        "\"\"\"\n",
        "mandelbrot_graph[i] = 1;\n",
        "pycuda::complex<float> c = lattice[i]; \n",
        "pycuda::complex<float> z(0,0);\n",
        "for (int j = 0; j < max_iters; j++)\n",
        "    {\n",
        "    \n",
        "     z = z*z + c;\n",
        "     \n",
        "     if(abs(z) > upper_bound)\n",
        "         {\n",
        "          mandelbrot_graph[i] = 0;\n",
        "          break;\n",
        "         }\n",
        "    }\n",
        "         \n",
        "\"\"\",\n",
        "\"mandel_ker\")\n",
        "\n",
        "\n",
        "\n",
        "def gpu_mandelbrot(width, height, real_low, real_high, imag_low, imag_high, max_iters, upper_bound):\n",
        "  real_vals = np.matrix(np.linspace(real_low, real_high, width), dtype=np.complex64)\n",
        "  imag_vals = np.matrix(np.linspace( imag_high, imag_low, height), dtype=np.complex64) * 1j\n",
        "  mandelbrot_lattice = np.array(real_vals + imag_vals.transpose(),\n",
        "dtype=np.complex64)\n",
        "  # copy complex lattice to the GPU\n",
        "  mandelbrot_lattice_gpu = gpuarray.to_gpu(mandelbrot_lattice)\n",
        "  # allocate an empty array on the GPU\n",
        "  mandelbrot_graph_gpu = gpuarray.empty(shape=mandelbrot_lattice.shape,\n",
        "  dtype=np.float32)\n",
        "  mandel_ker( mandelbrot_lattice_gpu, mandelbrot_graph_gpu, np.int32(max_iters), np.float32(upper_bound))\n",
        "  mandelbrot_graph = mandelbrot_graph_gpu.get()\n",
        "    \n",
        "  return mandelbrot_graph\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lcZzwTZw_5a",
        "outputId": "7ec16283-5a0f-4abf-c34d-8503dee3e277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "t1 = timeit.default_timer()\n",
        "mandel = gpu_mandelbrot(512,512,-2,2,-2,2,256, 2)\n",
        "t2 = timeit.default_timer()\n",
        "mandel_time = t2 - t1\n",
        "\n",
        "t1 = timeit.default_timer()\n",
        "fig = plt.figure(1)\n",
        "plt.imshow(mandel, extent=(-2, 2, -2, 2))\n",
        "plt.savefig('mandelbrot.png', dpi=fig.dpi)\n",
        "t2 = timeit.default_timer()\n",
        "\n",
        "dump_time = t2 - t1\n",
        "print( 'It took {} seconds to calculate the Mandelbrot graph.'.format(mandel_time))\n",
        "print('It took {} seconds to dump the image.'.format(dump_time))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It took 0.09711440799947013 seconds to calculate the Mandelbrot graph.\n",
            "It took 0.14270888200007903 seconds to dump the image.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAD8CAYAAACPd+p5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfV0lEQVR4nO3deXxU5b3H8c9vzkz2AGHfl7DjAsoWtOVypVfRtmDdit66t9StuFC91bbae22tVEW0Wqy2XuqVulRFsaBUqRYVWRXZCVuQsAUIZF9mee4fM2KATE5gTubMJL/365UXmZmHeZ7R5Ms5z3nO8xNjDEop1RCP2wNQSiU+DQqllC0NCqWULQ0KpZQtDQqllC0NCqWUrZiDQkR6iMgHIrJBRNaLyO31tBEReVJEtorIGhE5O9Z+lVLx43XgPQLANGPMZyKSDawSkfeMMRvqtLkQ6B/5Gg3MivyplEoCMR9RGGP2GmM+i3xfBmwEuh3XbBLwgglbCrQRkS6x9q2Uig8njiiOEpHewFnAsuNe6gbsqvO4MPLc3nreYwowBcDCGp5BKyeHqJSqo5oKak2N2LVzLChEJAt4HbjDGFN6qu9jjHkWeBaglbQ1o2W8QyNUSh1vmVnUqHaOXPUQER/hkJhjjHmjnia7gR51HnePPKeUSgJOXPUQ4M/ARmPMjCjN5gHXRK5+5AElxpgTTjuUUonJiVOPc4GrgbUisjry3H1ATwBjzDPAAuAiYCtQCVzvQL9KqTiJOSiMMR8DDU6GmPC97LfG2pdSyh26MlMpZUuDQillS4NCKWVLg0IpZUuDQillS4NCKWVLg0IpZUuDQillS4NCKWVLg0IpZUuDQillS4NCKWVLg0IpZUuDQillS4NCKWVLg0IpZUuDQillS4NCKWXLqV24nxeRIhFZF+X1cSJSIiKrI1/3O9GvUio+nKrrMRt4CnihgTYfGWO+41B/Sqk4cuSIwhizGCh24r2UUoknnnMUY0TkCxF5R0ROi2O/SqkYOVp7tAGfAb2MMeUichHwJuHK5ieoW3s0jYw4DU8p1ZC4HFEYY0qNMeWR7xcAPhFpH6Xts8aYEcaYET5S4zE8pZSNuASFiHSOlB5EREZF+j0Uj76VUrFz5NRDRF4CxgHtRaQQeADwwdGSgpcBN4tIAKgCJkeqhymlkoAjQWGMudLm9acIXz5VSiUhXZmplLKlQaGUsqVBoZSypUGhlLKlQaGUsqVBoZSypUGhlLKlQaGUsqVBoZSypUGhlLKlQaGUsqVBoZSypUGhlLKlQaGUsqVBoZSypUGhlLKlQaGUsqVBoZSypUGhlLIVr9qjIiJPishWEVkjImc70a+KI4+Ft1tXt0ehXOLUEcVsYEIDr19IuOBPf8LFfWY51K+Kp3DFBdUCxav26CTgBRO2FGgjIl2c6FvFSShIoHC326NQLonXHEU3YFedx4WR504gIlNEZKWIrPRTE5fBKaUalnCTmVpSUKnEE6+g2A30qPO4e+Q5pVQSiFdQzAOuiVz9yANKjDF749S3UipG8ao9ugC4CNgKVALXO9GvUio+4lV71AC3OtGXcl/5FXl4/IaMucvcHoqKk4SbzFSJrWxyHrN+N5N/e2AJctZpbg9HxYkGhTopwRShn9fDmKwtiDFuD0fFiSOnHqqZE6H4ujwOnR1CcqrZEQjyaXl/jgxuRZv8DEKVlW6PUDUxMQn8r0IraWtGy3i3h9FieXv3pGxoZ2paWTz34OOcmZJ2zOvloWqu2HIJcmWQwL79Lo1SxWKZWUSpKbZdm6+nHio6f4BdE2DS3f88ISQAsjxpzB3wFpun5bLnnnOwOnRwYZAqHjQoVFSBvftB4L72m6O2SRUfz186CwlA6PDhOI5OxZMGhYrKc8YAXjz/Gdt2Y9Mgc18IEwjEYVTKDRoUKqqaTpl0tRo3UblvbEhPPZoxDQoVVdr2Q+wMtGpU27afWwQPNbTTgEpmGhSqQa8Vj7RtM68ig84Ld0MoGIcRKTdoUKioglt3sOXWAayprY7exoS45/NLCRbuiePIVLxpUKiGrVjHzXffzrS9J25zus1fzuDF15N7826dyGzmdGVmSyaCWNbRX3Jv507g9RI8cBARITByMCW5aQQyYMFbeaRf4ufXHdcCkO+v4IL5d9HuMwv/6b3wrRf8g3tS09ZH5sI1YFmEKirc/HTKQboyswWzBvcn/5eZDPhNFew7wPbbB2IEOozcT9v0Sn7bay79fF5SxRf1PfwmyFZ/DVv87WljVfLqodEsWH86g3rtxX9/JzwffY54vRT9cCSd/3WQ4MYtcfyEyk5jV2bqEUUz5e3RHZPqI7itABk2BM+2XQRLS49pU3BpBz78xu+4aMI95E48wsq+jwPhFZdh6bb9+MRicEoGg1PCl1HHdltGTdePKQnVctkvfsCh8tOoKsxm6SWPkjd4Gv1v16BIRhoUzdThc7tz4wNvsrayO3uq9lHxgzYQCYrA+OEc6ZdCVZ9aSkIWc6f+jr6+LODEZdqnIlV8dLR8LD5j7tHnNtYKnZaCJy0NEwicMKfhSUvDBEMYf60jY1DO0qBopvaPhh+02kVq6334TZABv/wxA2/ax95bRzDh2iXc2u5jenqzaMxRgxN6eD2MnLaKt8cPxVPqpd9dS79+UYRDk88ikAYdV5VT1ieT1ovydV1GAtGgaIasfn1o27/46NyCTyy6ditm530jGDlhHdM7rQay4jqmLE8aT3ZdwSNdlrCu1nDHxz8hY+5yEA/5z5zNW+c/Tg8rxHtVXdhR05F3S8eR8q4GRaJwqqTgBBHZHCkZ+LN6Xr9ORA6IyOrI1w+d6FfVL7h1B/6F7Qma0NHnnhz4MuumPMULvRa7OLLwacnw1BSenvEEhfeOITh2KB9NCN/CnmNlcEVWCTur25H24VpXx6mOFXNQiIgFPE24bOAQ4EoRGVJP01eMMcMiX3+KtV/VsK4vb+H0P95G7j9uZE1tNcNTU7AkcZbNnJmSxtM3PsOe22rJPG5cm0s6YoKhKH9TucGJn5xRwFZjzHZjTC3wMuESgspFwUPFdP2khofGvMFpvhS3h1OvcekhXh/xLMWh0NGjn/JQNbtWdsMEdTl4InFijqK+coGj62l3qYiMBfKBO40xu+ppg4hMIVzImDQyHBhey+IZOphQuo/sR/bw027PkpdmkcgLcAenZFASqmLs2ssoLstEVmeT+7uVGL1vJKHE6yfobaC3MeZM4D3gL9EaaknB2BifxT1z5vBa3/cjIZH4WnvSeWnIC1QfSKfHb5boJdIE5ERQ2JYLNMYcMsZ8VXH4T8BwB/pVhPe1LL8ij/Ir8vB278aW/8ziG2nRb+JKVD29Wbxy4dPUXDgST5oz6zmUc5w49VgB9BeRPoQDYjJwVd0GItKlTgnBicBGB/pVQOXAjrz4yKP09GbQ790prL1gJqmSnL9oo1J9tL9vB1WbO8OefYSqky/wmquYjyiMMQHgNmAh4QB41RizXkT+R0QmRppNFZH1IvIFMBW4LtZ+VVjaks3c/eXFbAtUseL8J+osv05Oz/WeR/b/lVIyaZjbQ1F1OFVScAHh+qJ1n7u/zvf3Avc60VdC8Viub9YSKiuj+MGBbPlDO76dkfz/AudYGeS12c6O1IFuD0XVkbjT4UlAhg8Jh4WLrP65FFxlmkVIfOWOnAJ6TckHETwZX1/5sjp0wNurRwN/UzUVDYoYmBVrXTuiEF8KnjMHUTGoPW+Pe8qVMTSlruklMPoMSt/oTNWkUUC4HEBwjxYacoPe65GkKr57FtOmz6Ggtj2npcTnxq54eqTzMv7x4jq+nVHNhzM83Js5hVZ/XWr/F1WT0COKJJW14AvuXHQVd+QUuD2UJuETi4LaDrxe3opfbLmYNm+sdntILZoeUTjInDsM+aRpf6A92dmI5SEwuDd/n/AE8bpN3A27a3KYf/EoWhUdJKiXSl2lRxQOkiVfABwzAXc8T0YGBb8Zg+fMQafWh+Wh+NuDuXb235vlKUdd93VYzt5vdSJ4pMTtobR4GhROMgZvj+4U/KUv2x8egzWg7wlNSiaeydJrHqNwQtujz3kyMsIb2zZC8EgJbZcX0cEqtW+c5PwmRMU3K5AEvamtJdFTDyd5LDb9tgNfjJlF1rlp5Gb8mMGP1eLv3o6tV6UiNUK7gYfwY3hsynPclXcFlaVpiBVC9qfS96f2M/rWkAHsfNDH+PQamnvO51gZyLYMvfcjAWhQOCkUJOeDNMrGBkiVILT2E3jeMKvfU5E9Kb+SyfkZftblzQHCRXQeLR7IczKelCMeev9hU9Rt4Go6Z/PY0BcTam+JptRzTCEFD46h7xP5BA8ecns4LZZu1+80j8XeNwawZtRLfBkoj+xL2TiVoVrmVXTi4aevpOuc+sOi6JZzWPnzp1pMUHwZKOf3B7/JholdCRTutv8L6qQ0drv+lvHTFifezp0ov3QEN/T/FOCkQgIgw5PC5OzDLLlnJpse6H/Cqk9vbm+OjKxpMSEBMG7uT1k/OVdDwmUt5yeuqXgsguPORlJTKbool8UzZ8W8tiHDk8LS781gy+yhx4RFKCON687+NMYBJ4+gCUFOLcGtBW4PpcXToIhVKMjhAakcmduDy+5837F/7TtamXwy7kmKbv56szBPRRVXtF7pyPsng9JQNRzWKx6JQIPCAR1WlHJel3z+q52zVbC6eLO48yevUjY5D4CyoZ1oZyXunJLTUsVLmz6HESs5dupqzjQoYuTt0Z0t12bzUKc1TfL+17Q6yL4La8MFhQMQTODJZ6fNLD6Dzjcc1MujCUCDIkZlZ3fl2e8+16R9/G3sM8jw09iXZ9HlJCdIk1VRsIJtlR1AjyYSggbFyZJjryQVD/YyPr1pbzUfnprC5lvSwPYiVvMx8+AY9n0vi+D+IreHotAFV43m7dOLQIdW5N+QTtvPLTq9sgEsi34XbotL/2MGbqPHWYfj0pfbVtXU8o8/nEuH0qY5nVMnz5GgEJEJwBOABfzJGPPwca+nAi8Q3n37EPB9Y0yBE33Hg6SmsvGnnZn/7cfp50ul5jt+brp2AsU3tsPric8v76xeC0gTL+CLS39u8Zsgk/92OwMW7SVQUeH2cFREzEFRp6TgfxAu/rNCROYZYzbUaXYjcNgY009EJgPTge/H2ne9HN7HUrxeqs4fytKJM+hoZQLhvRJe7P0hM/6WyzkZWwjnY9Nq7Wned4pC+EjisndvY9CD6wiUlbk9HFWHE0cUR0sKAojIVyUF6wbFJOBXke9fA54SETEOrx8vvSqP/ef7GXTHVmduTRZh998G8M3ua46GRF13td1OPEKiJZi6ZySfP3Q2A95YhlYdTTxOTGbWV1KwW7Q2ke39S4B29b2ZiEwRkZUistJPTX1NopIQmJCzM36hkBA0LWgW0SUeMRidWk9YCfe/JpaSgtkvL2XAjauc2+jEGHp8fzOb/ucMioInni8/UtyXpdVaI9MJM7us5KlHniT/mVF4srPdHo46TlxKCtZtIyJeoDXhSU3nObwgyQQCpL/3BXlv3sX62ir8JkhJqIrJO87jg8vPZsae8x3tL5qSUBU1xh+XvtwyLDWV/O/OYsv9p+PN7e32cFQdTgTF0ZKCIpJCuKTgvOPazAOujXx/GfBPp+cnmpKpqWHwY3uZdtmPGPDWzYz/77sovTAARYeoDsbnKsTNOy/iF/tHxaUvN/nE4pXLnmDff3TBk3nivJByR8yTmcaYgIh8VVLQAp7/qqQgsNIYMw/4M/B/IrIVKCYcJkklUPAlFMCAVQLG8NUJx853z4H+Td//p5v6krLPxyPXf970nblseGoKF9z0CV/8vTshvUSaEByZozDGLDDGDDDG9DXG/Cby3P2RkMAYU22MudwY088YM+qrKyRJ6bgDoXYbAiyqatorH8tr/AycVQ1JcwwWuzvaf0rXN0qwOnV0eyiKBJzMTDbZn+1hyts/atI+Jv/rJsyq9XReGmRvoLxJ+0oUHa1M+qQfhFALSscEpkERo8CuQgbMLuW+/Wc2yfvPLu1I53d8YAzGC5a0nEu1d7Rdy/7nc3QX7gSgQeGAolGteW/3IB466GwF7r2Bcp548jKyXwmX0sv+Yj+Hgi0nKGpMgCPb2mKCegnabRoUsfJY5OTX0O7SncydeV54+zYHFAUrOPeDqXR8ZtnR50KZ6bxaMsKR908GOVYGtDu5RXeqaWhQxCoUxPrwM0xNDR3mb+Pfpt7MjOLcmN6yPFTNmNen0f/6L465b8VTWc3sVefEOuKkYg6nYPXv4/YwWjwNCgcF9xeROXcls7eOJmhCfHmSE4+VoVrmlLXjG9PvYuCD+Sfc3BbYXkDOCp9jRy3J4MOLH+OMl7fi7dHd7aG0aLofhcMO3TCK94c/Sog0xn0wlb7dD/B0v5cZ4Iu+eChoQkw/NJjnF55HSonQ+6/RCwDlbK7lvap0JmS0jEPy6/OvYvcn3cmt2uz2UFo0DQoneSwOj6smVTz4xIIjKaT8wnBbj1vYemUanhohZ1Ax84f+L5/VtOWu1ZdTFSkp6DmQQt+7w1vxNzR1l7qvjLtWX874MX8J99HMFS7tRu8HljT430Q1PQ0KJ4WCDLr3AKMfm0JoWxaD/reIwK5CZFch/ZeEm5RNzsM3VLj7jzfS45Hwk560NKR1q0b9MgQ35NP73lw+eCeN8zOa970fh4OVhHKrEF+KbrDrMg0KJ4kQ2FVIn2sOEqqurvcXv/Wbq8k7bRq57xQf3XchVF0N1dWN6sJq05riUR3ZF2gNHHRq5AnJJx6yP0rXkEgAOpnpIHPOUCDyix9FqLqa3r/8lNC6TafWRzBE2/kbmXPdRayvrTql90gWDx0YRedFRVg5OW4PpcXToHCQfLK6yfsIlZURPFKCZ3U+33n39ibvz009Uw8xZf5CKl9pjSctze3htGgaFEmq/KKhPD7+rzGv2UhUfhOkq+8wF2eW86u+8zhyyTC3h9SiSSJvC9FK2prRMt7tYSQk8aUgA3Op7NOK3//+Sc5MaV7/4k7dM5LNtwyi9L8rSflDO9LeXo54vSAenbNw0DKziFJTbHtfgB5RxGLUGcdUG48n468ltG4TmZsOMumDW10ZQ1PaV90KVqyjzaV7SXt7OQCenBysrp1cHlnLpEERi5UbHC0NcCqCW7bT5yVhfmXzOaKYebg3O/84AIwhVFl59PnggQMEdu5q4G+qpqJBEQuXQwLAk51N+/t30N93qN4NgJPN4WAlSw73xapN3FPilkiDIslVnzuI6T3eoq83ndELb6c81Lj1GInqxh2TqLg6i1ZvNv8t/5JJTAuuRKQt8ArQGygArjDGnFBjT0SCwNrIwy+NMRNj6Vd9LWPTfq6eNg2AIZ/uYljJHayf/HtSJflKDy6v8XP4od6k7lmDqWkZ97Iki1iPKH4GLDLG9AcWRR7Xp8oYMyzypSHhoEDBl2S+tozM15YR2L2Hfi+V86+qDLeHddJ2+MuZPP82Ut5doSGRgGINiknAXyLf/wW4OMb3UzGS2gAzrpzMxVsu4JPq5LgdvSRUxdUbryGjazlf3n+Obn2XgGJaRyEiR4wxbSLfC+FCxG3qaRcAVgMB4GFjzJsNvOcUYApAGhnDvyEXnfL4WjSPRWDcMK79w1v8Z3YRliTmdNTG2kpSJERvbwaWeCgPVXPWi3eSe9/yhJgsbu4cW0chIu+LyLp6vibVbRcp6BMtdXoZY0YAVwEzRaRvtP5iKSmovmZ1aMeesan8atlE1ifoAqVFVRaXrPgxbT2eo0GW5Umj54jdiNX8b6FPJraTmcaYb0V7TUT2i0gXY8xeEekCFEV5j92RP7eLyIfAWcC2Uxuyaow93+/Huh89FfkFTGN5jZ/hKVbCHFmsrqlh6p/upPuSKipGh6h729fA1kXssDw08wqKSSXWn5q6pQKvBd46voGI5IhIauT79sC5wIYY+1UNsPr1wXf+wWNC4c7N3+eMWbfxg4Jx7g2M8HZ/q2pqmXrnT+j+2yV4Fq/mmwvuYnVNDQeDFcwpa0ef9ANU/fsZro5THSvW/SgeBl4VkRuBncAVACIyArjJGPNDYDDwRxEJEQ6mh40xGhRNKLh1B8X5edSc5SdVfPhNkD272zJw+krWlo3g7mtLuKXdR/TxZcVtTCWhKn6+dxzzV5+JVWLR981wCQKMYcDNK5hy7R0EU6HDqjLKe2fSamm+7mqVQPSmsGaqbHIe1z0wj3UV3SisbEPNNenh+qlA4LzhHOmXQvEoP3//1u/xSajBPT2dkO+v4Np7p9H69c8hGMQEAse87klLwwQCJzyvmlZjJzN1h6tmKufjXbz53dEEtxXgGdoBKS48+pr3n6to/0+o6ngObT1BLph5Dz0n7uDlfnOB8IRiLGqMn5JQLZesv5pDZZnUFmby8aWPUjQKWv21/jUSDW32o9ynRxQtmDW4P5vvy2Lgw5Ww7wA7fjIII9BmZBFt0qr4Xe7r9PN6yPBEX9fgN0Hy/bVsqu1EO6ucV4tH8c7a0xnYey/BBzri+ehzxOvlwA0j6fSvAwQ3b43jJ1R2GntEoUHRwonXe/Rw3+rUEfF6CRYdRCwPweGDKMlNJ5ABVR2FSy79iF93DK/Ez/dXcMH8O2m/0qLNlmp8G3biH9yT2pwUMt5bAx7PMXd+qsSkQaGcIULZFaM597+W8UjnY2/U2uEv54JPb6HfbbsJHjzk0gBVLHTjGuWM0Wfw3PTHTwgJgD6+LDZ+czY7ZnUN7z6lmi0NChWV1T+XwU9v4LSU9OhtxMP0Ya9j9egWx5GpeNOgUNEZw8VtPrNtNjGzkr0X6FFFc6ZBoaKq6tuOXt7SRrU9PCyIR+tvNFsaFCqqtL3l7Ak2bm+Lzos9BA8caOIRKbdoUKioQuu28IOFN9m2+7DKQ0UXj556NGMaFCoqb9fOIPDrg4Oitqkxfn70+o8xAp42reM4OhVPGhQqOq9Fz/nw9+njWFN74hLr8lA1Ezd9jwEzttP10SW6lqIZ02NFFVWg4EvSCr4kTYQf+u7k4FkGcmp5e+zTzDk8mg+nn0PreWsI6ArMZk+PKJQ9Y8iZ/Sn9b19K+0Wp9PN5GZu9mdabSnWZdguhQaFOirfasLE2xOKygW4PRcWRnnqok5L9ylJuDU3F8hvSVy93ezgqTjQo1EnL+tsyt4eg4kxPPZRStmIKChG5XETWi0gosk9mtHYTRGSziGwVkWjVxJRSCSrWI4p1wCXA4mgNRMQCngYuBIYAV4rIkBj7VUrFUUxzFMaYjQDhImFRjQK2GmO2R9q+TLgUoe7ErVSSiMccRTdgV53HhZHn6iUiU0RkpYis9KPFapVKBLZHFCLyPtC5npd+bow5oeBPrIwxzwLPQngrPKffX50a8XqxOnciULjb7aEoF8RUUrCRdgM96jzuHnlOJRETMpDA+6uqphWPU48VQH8R6SMiKcBkwqUIVTIJBQns3uP2KJRLYr08+j0RKQTGAPNFZGHk+a4isgDAGBMAbgMWAhuBV40x62MbtlIqnnS7fqVaMN2uXynlGA0KpZQtDQqllC0NCqWULQ0KpZQtDQqllC0NCqWULQ0KpZQtDQqllC0NCqWULQ0KpZQtDQqllC0NCqWULQ0KpZQtDQqllC0NCqWULQ0KpZQtDQqllK14lRQsEJG1IrJaRFbG0qdSKv5irWb+VUnBPzai7b8bYw7G2J9SygXxKCmolEpy8ZqjMMA/RGSViEyJU59KKYfEq6TgN4wxu0WkI/CeiGwyxtRbAT0SJFMA0sho5NsrpZpSPEoKYozZHfmzSETmEq5wXm9QaO1RpRJPk596iEimiGR/9T1wPuFJUKVUkmjykoJAJ+BjEfkCWA7MN8a8G0u/Sqn40pKCSrVgWlJQKeUYDQqllC0NCqWULQ0KpZQtDQqllC0NCqWULQ0KpZQtDQqllC0NCqWULQ0KpZQtDQqllC0NCqWULQ0KpZQtDQqllC0NCqWULQ0KpZQtDQqllC0NCqWULQ0KpZStWDfXfURENonIGhGZKyJtorSbICKbRWSriPwslj6VUvEX6xHFe8DpxpgzgXzg3uMbiIgFPA1cCAwBrhSRITH2q5SKo5iCwhjzD2NMIPJwKdC9nmajgK3GmO3GmFrgZWBSLP0qpeIr1mrmdd0AvFLP892AXXUeFwKjo71J3ZKCQM375rXmWCyoPdAcK7s3188FzfezDWxMI0dqj4rIz4EAMOdkRlifuiUFRWSlMWZErO+ZaPRzJZ/m+tlEZGVj2sVce1RErgO+A4w39VcT2g30qPO4e+Q5pVSSiPWqxwTgHmCiMaYySrMVQH8R6SMiKcBkYF4s/Sql4ivWqx5PAdnAeyKyWkSegWNrj0YmO28DFgIbgVeNMesb+f7Pxji+RKWfK/k018/WqM+V0LVHlVKJQVdmKqVsaVAopWwldFA0dol4MhKRy0VkvYiERCTpL7s112X6IvK8iBSJSLNazyMiPUTkAxHZEPk5vL2h9gkdFDRiiXgSWwdcAix2eyCxaubL9GcDE9weRBMIANOMMUOAPODWhv6fJXRQNHKJeFIyxmw0xmx2exwOabbL9I0xi4Fit8fhNGPMXmPMZ5HvywhfkewWrX1CB8VxbgDecXsQql71LdOP+kOnEouI9AbOApZFa+PkvR6nJN5LxOOpMZ9NKTeJSBbwOnCHMaY0WjvXg8KBJeIJy+6zNSO6TD8JiYiPcEjMMca80VDbhD71aOQSceU+XaafZEREgD8DG40xM+zaJ3RQEGWJeHMgIt8TkUJgDDBfRBa6PaZTFeMy/YQmIi8BnwIDRaRQRG50e0wOORe4Gjgv8ru1WkQuitZYl3ArpWwl+hGFUioBaFAopWxpUCilbGlQKKVsaVAopWxpUCilbGlQKKVs/T8RftB5jDSYrwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO1EMX-T2BSW"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "In the pervious code we create matrix instead on nxn, which is divided among cuda threads across SM. In each SM core, addition operation is performed in parallel before serailizing over the single loop i.e to find Z.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMj3VD6K4g3C"
      },
      "source": [
        "# A brief foray into functional programming\n",
        "\n",
        "\n",
        "---\n",
        "Before we continue, let's briefly do a review of two functions available in Python for\n",
        "functional programming—\n",
        "1.   map\n",
        "2.   reduce\n",
        "\n",
        "\n",
        "---\n",
        "We see that map acts as ElementwiseKernel! This is actually a standard design pattern in\n",
        "functional programming. Now, let's look at reduce; rather than taking in a list and\n",
        "outputting a directly corresponding list, reduce takes in a list, performs a recursive binary\n",
        "operation on it, and outputs a singleton. Let's get a notion of this design pattern by typing\n",
        "reduce(lambda x, y : x + y, [1,2,3,4]). When we type this in IPython, we will\n",
        "see that this will output a single number, 10, which is indeed the sum of 1+2+3+4. You can\n",
        "try replacing the summation above with multiplication, and seeing that this indeed works\n",
        "for recursively multiplying a long list of numbers together. Generally speaking, we use\n",
        "reduce operations with associative binary operations; this means that, no matter the order we\n",
        "perform our operation between sequential elements of the list, will always invariably give\n",
        "the same result, provided that the list is kept in order. (This is not to be confused with the\n",
        "commutative property.)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f54nNKBB1kcb"
      },
      "source": [
        "# Example : Lambda func : Used to define single line function.\n",
        "pow = lambda x: x**2"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsVExhD57ADL",
        "outputId": "02f313a1-da53-4e77-9bad-8ca0fb2d7645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pow(2),pow(22),pow(12)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 484, 144)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urzOzbZC7A0h",
        "outputId": "6b7ccad5-a206-4f6f-8eef-479c6b998bc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Example : Map func : Takes 2 input : a) function b) *args\n",
        "[x for x in map(lambda x: x**2,[2,3,4,5])]"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 9, 16, 25]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04h5TQHaA71W",
        "outputId": "34f81039-fa8a-49e7-e3a0-dec8931ddc9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Example : Map func : Takes 2 input : a) function b) *args\n",
        "import functools\n",
        "\n",
        "# map(functools.partial(add, y=2), a)\n",
        "[x for x in map(lambda x,y: x**2+y,[2,3,0,1],[3,0,4,5])]"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7, 9, 4, 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsBkom4z731g",
        "outputId": "699865c9-dbda-47ab-9f6e-5b3ccd050671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Example : List func : \n",
        "from functools import reduce\n",
        "'''\n",
        "Working : \n",
        "\n",
        "At first step, first two elements of sequence are picked and the result is obtained.\n",
        "Next step is to apply the same function to the previously attained result and the number just succeeding the second element and the result is again stored.\n",
        "This process continues till no more elements are left in the container.\n",
        "The final returned result is returned and printed on console.\n",
        "1+2 = 3 |1,2|\n",
        "9+3 = 12|3,3|\n",
        "144+4 = 148 |12,4|\n",
        "'''\n",
        "reduce(lambda x, y : x**2 + y, [1,2,3,4])"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "148"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFcJgZAkGq7Q"
      },
      "source": [
        "# Parallel scan and reduction kernel basics\n",
        "\n",
        "\n",
        "---\n",
        "Let's look at a basic function in PyCUDA that reproduces the functionality of\n",
        "reduce—InclusiveScanKernel. (You can find the code under the\n",
        "simple_scankernal0.py filename.) Let's execute a basic example that sums a small list of\n",
        "numbers on the GPU:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcEhOFeB94nf",
        "outputId": "adef4ef1-3743-4d77-ed3e-cc81ffd4ea6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## Example 1\n",
        "import numpy as np\n",
        "import pycuda.autoinit\n",
        "from pycuda import gpuarray\n",
        "from pycuda.scan import InclusiveScanKernel\n",
        "seq = np.array([1,2,3,4],dtype=np.int32)\n",
        "seq_gpu = gpuarray.to_gpu(seq)\n",
        "sum_gpu = InclusiveScanKernel(np.int32, \"a+b\")\n",
        "print(sum_gpu(seq_gpu).get())\n",
        "print(np.cumsum(seq))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1  3  6 10]\n",
            "[ 1  3  6 10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlOiShvyF15E",
        "outputId": "859ae6c2-0138-4ea0-f441-879bd16e9513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## Example 2\n",
        "import numpy as np\n",
        "import pycuda.autoinit\n",
        "from pycuda import gpuarray\n",
        "from pycuda.scan import InclusiveScanKernel\n",
        "seq = np.array([1,100,-3,-10000, 4, 10000, 66, 14, 21],dtype=np.int32)\n",
        "seq_gpu = gpuarray.to_gpu(seq)\n",
        "max_gpu = InclusiveScanKernel(np.int32, \"a > b ? a : b\")\n",
        "print(max_gpu(seq_gpu).get())\n",
        "print(np.max(seq))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    1   100   100   100   100 10000 10000 10000 10000]\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuJ9KpQ3OVsy"
      },
      "source": [
        "Now, let's finally look one more PyCUDA function for generating GPU\n",
        "kernels—ReductionKernel. Effectively, **ReductionKernel** acts like a\n",
        "ElementwiseKernel function followed by a parallel scan kernel. What algorithm is a good\n",
        "candidate for implementing with a ReductionKernel? The first that tends to come to\n",
        "mind is the dot product from linear algebra. Let's remember computing the dot product of\n",
        "two vectors has two steps:\n",
        "1. Multiply the vectors pointwise\n",
        "2. Sum the resulting pointwise multiples\n",
        "\n",
        "These two steps are also called multiply and accumulate. Let's set up a kernel to do this\n",
        "computation now:\n",
        "\n",
        "## READ\n",
        "---\n",
        "\n",
        "\n",
        "dot_prod = ReductionKernel(np.float32, neutral=\"0\", reduce_expr=\"a+b\",\n",
        "map_expr=\"vec1[i]*vec2[i]\", arguments=\"float *vec1, float *vec2\")\n",
        "\n",
        "\n",
        "---\n",
        "## END\n",
        "First, note the datatype we use for our kernel (a float32). We then set up the input\n",
        "arguments to our CUDA C kernel with arguments, (here two float arrays representing\n",
        "each vector designated with float *) and set the pointwise calculation with map_expr,\n",
        "here it is pointwise multiplication. As with ElementwiseKernel, this is indexed over i.\n",
        "We set up reduce_expr the same as with InclusiveScanKernel. This will take the\n",
        "resulting output from the element-wise operation and perform a reduce-type operation on\n",
        "the array. Finally, we set the neutral element with neutral. This is an element that will act as\n",
        "an identity for reduce_expr; here, we set neutral=0, because 0 is always the identity\n",
        "under addition (under multiplication, one is the identity). We'll see why exactly we have to\n",
        "set this up when we cover parallel prefix in greater depth later in this book."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-EmLLFANvUz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}