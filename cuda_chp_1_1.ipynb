{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cuda_chp_1_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM+XE4W8TuZ0K15HN9MxS4e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nile649/CUDA_Tutorials/blob/master/cuda_chp_1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7I2Kcyx1k_6",
        "outputId": "123aed95-2bc3-496e-bc18-4024a346fa8c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Dec  5 03:02:12 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yb68Fr41uob"
      },
      "source": [
        "# %%capture\n",
        "!sudo apt update\n",
        "!sudo add-apt-repository ppa:graphics-drivers\n",
        "!sudo apt-key adv --fetch-keys  http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
        "!sudo bash -c 'echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/cuda.list'\n",
        "!sudo bash -c 'echo \"deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/cuda_learn.list'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUxAF_9Y2kRI"
      },
      "source": [
        "# %%capture\n",
        "!sudo apt install cuda-10-1\n",
        "!sudo apt install libcudnn7"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVqfnkAL2k1o"
      },
      "source": [
        "%%capture\n",
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM2n3P7I2pYF",
        "outputId": "f28e13c8-ec1f-4cb5-fb3e-6f9d146f79d6"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR8X34fK7MuU"
      },
      "source": [
        "# Intro to GPU using CUDA programming paradigm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXNfIZCO3P9n"
      },
      "source": [
        "CPU : Host \n",
        "GPU : Device\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "CPU is focused on increasing latency, and whereas GPU focuses on increasing the throughput.\n",
        "\n",
        "Latency vs Bandwidth\n",
        "\n",
        "Example :\n",
        "We have to travel from LA to virginia. Distance of 45000 km.\n",
        "\n",
        "Ferarri car capacity of 2 people and speed of 200km/hr.\n",
        "Bus capacity of 40 people and speed of 50km/hr.\n",
        "\n",
        "CAR\n",
        "Latency    : 22.5 hr | Hour\n",
        "Throughput : 0.089   | People / Hour (Number of threads / Latency)\n",
        "\n",
        "Bus\n",
        "Latency    : 90 hr | Hour\n",
        "Throughput : 0.55   | People / Hour (Number of threads / Latency)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "CPU already has \n",
        "  1. complex hardware\n",
        "  2. Expensive in terms of power consumption \n",
        "  3. Flexibility and performance \n",
        "\n",
        "GPU is made for\n",
        "  1. Lots of simple ALU.\n",
        "  2. Explicitly parallel programming model.\n",
        "  3. Optimize throughput not latency.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLZuEGcI7heq"
      },
      "source": [
        "CUDA program are serial program written for a single thread. CPU is responsible to invoke the kernel (The serial code for a single thread is called kernel),  CPU also manages how many threads we need to execute.\n",
        "\n",
        "The GOAL is always to maximize the use of thread or itâ€™s waste of GPU potenial.\n",
        "\n",
        "# CUDA Program 1.1 | Square"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zvux2ZP_Mz7"
      },
      "source": [
        "int main(int argc, char** argv)\n",
        "\n",
        "similar to python where ** determines dictionary args.\n",
        "\n",
        "```\n",
        "argc will be the number of strings pointed to by argv. This will (in practice) be 1 plus the number of arguments, as virtually all implementations will prepend the name of the program to the array.\n",
        "\n",
        "The variables are named argc (argument count) and argv (argument vector) by convention, but they can be given any valid identifier: int main(int num_args, char** arg_strings) is equally valid.\n",
        "\n",
        "They can also be omitted entirely, yielding int main(), if you do not intend to process command line arguments.\n",
        "\n",
        "Try the following program:\n",
        "\n",
        "#include <iostream>\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    std::cout << \"Have \" << argc << \" arguments:\" << std::endl;\n",
        "    for (int i = 0; i < argc; ++i) {\n",
        "        std::cout << argv[i] << std::endl;\n",
        "    }\n",
        "}\n",
        "Running it with ./test a1 b2 c3 will output\n",
        "\n",
        "Have 4 arguments:\n",
        "./test\n",
        "a1\n",
        "b2\n",
        "c3\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3jIgSCuIzMW"
      },
      "source": [
        "# CUDA core functions \n",
        "\n",
        "compiler :\n",
        "*   filename: square.cu\n",
        "*   compile:  nvcc -o square square.cu\n",
        "\n",
        "\n",
        "functions :\n",
        "\n",
        "float *d_in;\n",
        "\n",
        "\n",
        "\n",
        "*   cudaMalloc [Cuda memory allocation]\n",
        "\n",
        "    cudaMalloc( (void**) &d_in, ARRAY_BYTES);\n",
        "\n",
        "*   cudaMemcpy(d_in, h_in, ARRAY_BYTES, cudaMemcpyHostToDevice);\n",
        "\n",
        "*   Transfer of variables CPU <--> CUDA\n",
        "    1. cudaMemcpyHostToDevice [copy var from CPU to GPU]\n",
        "    2. cudaMemcpyDeviceToHost [copy var from GPU to CPU]\n",
        "    3. cudaMemcpyDeviceToDevice [copy var from GPU to GPU]\n",
        "\n",
        "*   _kernel_name_<<<Blocks, threads>>>(argc,argv)\n",
        "\n",
        "*   cudaDeviceSynchronize(); synchronization of threads before proceeding fruther.\n",
        "\n",
        "```\n",
        "  maximum number of threads/block is 512 (older GPUs) or 1024 (newer GPUs)\n",
        "\tfor instance:\n",
        "\t\t128 threads? \n",
        "\t\t\tsquare<<<1,  '128>>>( ... )\tOK\n",
        "\t\t1280 threads? \n",
        "\t\t\tsquare<<<10, 128>>>( ... )\tOK\n",
        "\t\t\tsquare<<<5,  256>>>( ... )\tOK\n",
        "\t\t\tsquare<<<1, 1280>>>( ... ) !!!NO!!!\n",
        "\tFor 2-dimensional problem, like image processing, use <<<128, 128>>>\n",
        "\ton a 128x128 pixels image.\n",
        "\t\n",
        "\tkernel<<< grid of blocks , block of threads >>> ( ... )\n",
        "\t\t\t\t1,2 or 3D       1,2 or 3D\n",
        "\tdim3( x, y, z )\n",
        "\tdim3( x, 1, 1 ) == dim3( w ) == w\n",
        "\t\n",
        "\tkernel<<< dim3( bx, by, bz ) , dim3( tx, ty, tz ), shmem >>> ( ... )\n",
        "\t\n",
        "\tbx * by * bz\tgrid of blocks\n",
        "\ttx * ty * tz\tblock of threads\n",
        "\tshmem\t\t\tmemory shared per block (in bytes)\n",
        "\t\n",
        "\teach thread knows it's threadId\n",
        "\t\n",
        "\tthreadId\tthread within a block\n",
        "\t\tthreadId.x\n",
        "\t\tthreadId.y\n",
        "\tblockDim\tsize of a block\n",
        "\tblockIdx\tblock within a grid\n",
        "\tgridDim\t\tsize of a grid\n",
        "\t\n",
        "\t\n",
        "\tMap concept\n",
        "\t\n",
        "\tMap:\n",
        "\t\t- set of elements (eg. 64 float array)\n",
        "\t\t- function to run on each element indipendently\t(eg. square function)\n",
        "\tso Map is:\n",
        "\t\tmap(elements,function)\n",
        "\t\n",
        "\tGPUs are good at map because:\n",
        "\t\t- gpus have many parallel processors\n",
        "\t\t- gpus optimize for throughput\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uVfjlZtONp7"
      },
      "source": [
        "# why use type caste (void**)\n",
        "\n",
        "hmm, we will try to draw analogy from c++\n",
        "\n",
        "DS* var = new DS()\n",
        "int* var = new int[5]\n",
        "\n",
        "the cudaMalloc functions look like this\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "void myMalloc(void** ptr, int size) {\n",
        "\n",
        "  *ptr = malloc(size);\n",
        "\n",
        "  return;\n",
        "\n",
        "}\n",
        "\n",
        "int* ptr = NULL;\n",
        "\n",
        "myMalloc((void**) &ptr, size);\n",
        "\n",
        "ptr[0] = 0;\n",
        "\n",
        "we are trying to create array of memories for pointers pointing to a data location.\n",
        "\n",
        "if *ptr > var\n",
        "then **pts > [ptr]\n",
        "\n",
        "thats a reason why we explicity type cast before the address of a pointer.\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS4_CARz2zzO",
        "outputId": "4d165f75-2300-448d-fc2b-64913a63b3d4"
      },
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        "\tDevice (GPU) code\n",
        "\t__global__ - declaration specifier - GPU code mark\n",
        "*/\n",
        "\n",
        "__global__ void square(float *d_out, float *d_in) {\n",
        "\tint idx = threadIdx.x;\n",
        "\tfloat f = d_in[idx];\n",
        "\td_out[idx] = f * f;\n",
        "}\n",
        "\n",
        "/*\n",
        "\tHost (CPU) code\n",
        "*/\n",
        "int main(int argc, char** argv) {\n",
        "\tconst int ARRAY_SIZE = 64;\n",
        "\tconst int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\t\n",
        "\t// generate the input array on the host\n",
        "\n",
        "\tfloat h_in[ARRAY_SIZE];\n",
        "\tfor( int i=0; i<ARRAY_SIZE; i++) {\n",
        "\t\th_in[i] = float(i);\n",
        "\t}\n",
        "\t\n",
        "\tfloat h_out[ARRAY_SIZE];\n",
        "\t\n",
        "\t// declare GPU memory pointers\n",
        "\tfloat* d_in;\n",
        "\tfloat* d_out;\n",
        "\t\n",
        "\t// allocate GPU memory\n",
        "\tcudaMalloc( (void**) &d_in, ARRAY_BYTES);\n",
        "\tcudaMalloc( (void**) &d_out, ARRAY_BYTES);\n",
        "\t\n",
        "\n",
        "\tcudaMemcpy(d_in, h_in, ARRAY_BYTES, cudaMemcpyHostToDevice);\n",
        "\n",
        "\tsquare<<<1, ARRAY_SIZE>>>(d_out, d_in);\n",
        "\t\n",
        "\tcudaDeviceSynchronize();\n",
        "\t\n",
        "\t// copy back the result array to the CPU\n",
        "\tcudaMemcpy(h_out, d_out, ARRAY_BYTES, cudaMemcpyDeviceToHost);\n",
        "\t\n",
        "\t// print out the resulting array\n",
        "\tfor( int i=0; i<ARRAY_SIZE; i++) {\n",
        "\t\tprintf(\"%f\", h_out[i]);\n",
        "\t\tprintf(((i%4) != 3) ? \"\\t\" : \"\\n\");\n",
        "\t}\n",
        "\t\n",
        "\t// free the GPU memory allocation\n",
        "\tcudaFree(d_in);\n",
        "\tcudaFree(d_out);\n",
        "\t\n",
        "\treturn 0;\n",
        "}"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.000000\t1.000000\t4.000000\t9.000000\n",
            "16.000000\t25.000000\t36.000000\t49.000000\n",
            "64.000000\t81.000000\t100.000000\t121.000000\n",
            "144.000000\t169.000000\t196.000000\t225.000000\n",
            "256.000000\t289.000000\t324.000000\t361.000000\n",
            "400.000000\t441.000000\t484.000000\t529.000000\n",
            "576.000000\t625.000000\t676.000000\t729.000000\n",
            "784.000000\t841.000000\t900.000000\t961.000000\n",
            "1024.000000\t1089.000000\t1156.000000\t1225.000000\n",
            "1296.000000\t1369.000000\t1444.000000\t1521.000000\n",
            "1600.000000\t1681.000000\t1764.000000\t1849.000000\n",
            "1936.000000\t2025.000000\t2116.000000\t2209.000000\n",
            "2304.000000\t2401.000000\t2500.000000\t2601.000000\n",
            "2704.000000\t2809.000000\t2916.000000\t3025.000000\n",
            "3136.000000\t3249.000000\t3364.000000\t3481.000000\n",
            "3600.000000\t3721.000000\t3844.000000\t3969.000000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI5Nk41FGzJU",
        "outputId": "754b0349-f837-4f5b-9da5-2d6c80c1c2f2"
      },
      "source": [
        "%%cu\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void cube(float *d_out, float *d_in){\n",
        "    int idx = threadIdx.x;\n",
        "    float f = d_in[idx];\n",
        "    d_out[idx] = f*f*f;\n",
        "}\n",
        "/*\n",
        "d_var indicates device variables.\n",
        "h_var indicates host variables.\n",
        "*/\n",
        "int main(int argc, char **argv){\n",
        "    \n",
        "    const int ARRAY_SIZE = 96;\n",
        "    const int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);   \n",
        "\n",
        "    float h_in[ARRAY_SIZE];\n",
        "    float h_out[ARRAY_SIZE];\n",
        "\n",
        "    for(int i=0;i<ARRAY_SIZE;i++){\n",
        "        h_in[i] = float(i);\n",
        "        // printf(\"%f \\n\",h_in[i]);\n",
        "    }\n",
        "\n",
        "    // declare GPU variables as we will do for a single thread;\n",
        "\n",
        "    float *d_in;\n",
        "    float *d_out;\n",
        "\n",
        "    // allocate GPU memory\n",
        "\n",
        "    cudaMalloc((void**) &d_in, ARRAY_BYTES);\n",
        "    cudaMalloc((void**) &d_out, ARRAY_BYTES);\n",
        "\n",
        "    // transfer CPU arrays to GPU.\n",
        "\n",
        "    cudaMemcpy(d_in, h_in, ARRAY_BYTES, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // launch the kernel\n",
        "\t  cube<<<1, ARRAY_SIZE>>>(d_out, d_in);\n",
        "\n",
        "    // transfer GPU arrays to CPU.\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaMemcpy(h_out, d_out, ARRAY_BYTES, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    \n",
        "    // print out the resulting array\n",
        "    for (int i =0; i < ARRAY_SIZE; i++) {\n",
        "      printf(\"%f\", h_out[i]);\n",
        "      printf(((i % 4) != 3) ? \"\\t\" : \"\\n\");\n",
        "    }\n",
        "\n",
        "    cudaFree(d_in);\n",
        "    cudaFree(d_out);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.000000\t1.000000\t8.000000\t27.000000\n",
            "64.000000\t125.000000\t216.000000\t343.000000\n",
            "512.000000\t729.000000\t1000.000000\t1331.000000\n",
            "1728.000000\t2197.000000\t2744.000000\t3375.000000\n",
            "4096.000000\t4913.000000\t5832.000000\t6859.000000\n",
            "8000.000000\t9261.000000\t10648.000000\t12167.000000\n",
            "13824.000000\t15625.000000\t17576.000000\t19683.000000\n",
            "21952.000000\t24389.000000\t27000.000000\t29791.000000\n",
            "32768.000000\t35937.000000\t39304.000000\t42875.000000\n",
            "46656.000000\t50653.000000\t54872.000000\t59319.000000\n",
            "64000.000000\t68921.000000\t74088.000000\t79507.000000\n",
            "85184.000000\t91125.000000\t97336.000000\t103823.000000\n",
            "110592.000000\t117649.000000\t125000.000000\t132651.000000\n",
            "140608.000000\t148877.000000\t157464.000000\t166375.000000\n",
            "175616.000000\t185193.000000\t195112.000000\t205379.000000\n",
            "216000.000000\t226981.000000\t238328.000000\t250047.000000\n",
            "262144.000000\t274625.000000\t287496.000000\t300763.000000\n",
            "314432.000000\t328509.000000\t343000.000000\t357911.000000\n",
            "373248.000000\t389017.000000\t405224.000000\t421875.000000\n",
            "438976.000000\t456533.000000\t474552.000000\t493039.000000\n",
            "512000.000000\t531441.000000\t551368.000000\t571787.000000\n",
            "592704.000000\t614125.000000\t636056.000000\t658503.000000\n",
            "681472.000000\t704969.000000\t729000.000000\t753571.000000\n",
            "778688.000000\t804357.000000\t830584.000000\t857375.000000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}